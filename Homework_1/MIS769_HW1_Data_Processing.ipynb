{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# üìä Homework 1: Introduction to Data Processing\n**MIS 769 - Big Data Analytics for Business | Spring 2026**\n\n**Points:** 20 | **Due:** Sunday, February 2, 2026 @ 11pm Pacific\n\n**Author:** Richard Young, Ph.D. | UNLV Lee Business School\n\n**Compute:** CPU (free tier)\n\n---\n\n## What You'll Learn\n\n1. Connect Google Colab to external data sources (Kaggle/HuggingFace)\n2. Load and explore large datasets with pandas\n3. Perform **Data Quality Assessment** (missing values, duplicates, outliers)\n4. **Find Something Interesting** in your data\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 1: Environment Setup (3 points)\n",
    "\n",
    "First, let's install the libraries we need and verify everything works."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Install required packages\n",
    "!pip install datasets pandas numpy matplotlib seaborn -q\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")\n",
    "print(f\"   Pandas version: {pd.__version__}\")\n",
    "print(f\"   NumPy version: {np.__version__}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Part 2: Load Your Data (4 points)\n",
    "\n",
    "Choose **ONE** of the following data sources. HuggingFace is recommended for beginners (no login required).\n",
    "\n",
    "### Option A: HuggingFace Datasets (Easiest - No Login Required)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# OPTION A: Load from HuggingFace (RECOMMENDED)\nfrom datasets import load_dataset\n\n# Choose ONE dataset by uncommenting:\n\n# NVIDIA HelpSteer2 - AI Response Quality Ratings (~21k rows)\ndataset = load_dataset(\"nvidia/HelpSteer2\", split=\"train\")\n\n# OR: Movie Reviews - IMDB\n# dataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n\n# OR: Yelp Reviews\n# dataset = load_dataset(\"Yelp/yelp_review_full\", split=\"train[:50000]\")\n\n# Convert to pandas DataFrame\ndf = dataset.to_pandas()\n\nprint(f\"‚úÖ Loaded {len(df):,} records from HuggingFace\")\nprint(f\"   Columns: {list(df.columns)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Option B: Kaggle Datasets (More Variety - Requires API Key)\n",
    "\n",
    "If you want to use Kaggle, you'll need to:\n",
    "1. Create a Kaggle account at kaggle.com\n",
    "2. Go to Settings ‚Üí API ‚Üí Create New Token\n",
    "3. Upload the `kaggle.json` file when prompted below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# OPTION B: Load from Kaggle (uncomment to use)\n",
    "\n",
    "# # Step 1: Set up Kaggle credentials\n",
    "# !pip install kaggle -q\n",
    "# from google.colab import files\n",
    "# print(\"Upload your kaggle.json file:\")\n",
    "# files.upload()  # Upload kaggle.json\n",
    "\n",
    "# !mkdir -p ~/.kaggle && cp kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# # Step 2: Download a dataset (choose one)\n",
    "# # Spotify Tracks:\n",
    "# !kaggle datasets download -d maharshipandya/-spotify-tracks-dataset -p /content --unzip\n",
    "# df = pd.read_csv('/content/dataset.csv')\n",
    "\n",
    "# # OR: Netflix Titles:\n",
    "# # !kaggle datasets download -d shivamb/netflix-shows -p /content --unzip\n",
    "# # df = pd.read_csv('/content/netflix_titles.csv')\n",
    "\n",
    "# print(f\"‚úÖ Loaded {len(df):,} records from Kaggle\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Verify Your Data\n",
    "\n",
    "Let's take a first look at the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Basic info about your dataset\nprint(\"=\" * 60)\nprint(\"DATASET OVERVIEW\")\nprint(\"=\" * 60)\nprint(f\"Number of rows: {len(df):,}\")\nprint(f\"Number of columns: {len(df.columns)}\")\nprint(f\"\\nColumn names: {list(df.columns)}\")\nprint(f\"\\nData types:\")\nprint(df.dtypes)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Preview first few rows\n",
    "df.head()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Part 3: Data Quality Assessment (8 points)\n",
    "\n",
    "A critical skill for any data professional is assessing data quality BEFORE analysis. Let's check for common issues.\n",
    "\n",
    "### 3.1 Missing Values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for missing values\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing.index,\n",
    "    'Missing Count': missing.values,\n",
    "    'Missing %': missing_pct.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing %', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(\"\\n‚ö†Ô∏è Columns with missing values:\")\n",
    "    print(missing_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n‚úÖ No missing values found!\")\n",
    "\n",
    "print(f\"\\nTotal cells: {df.size:,}\")\n",
    "print(f\"Missing cells: {df.isnull().sum().sum():,}\")\n",
    "print(f\"Completeness: {(1 - df.isnull().sum().sum() / df.size) * 100:.2f}%\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Duplicate Records"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for duplicate rows\n",
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATE RECORDS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "duplicates = df.duplicated().sum()\n",
    "duplicate_pct = (duplicates / len(df) * 100)\n",
    "\n",
    "print(f\"\\nTotal rows: {len(df):,}\")\n",
    "print(f\"Duplicate rows: {duplicates:,}\")\n",
    "print(f\"Duplicate percentage: {duplicate_pct:.2f}%\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"\\n‚ö†Ô∏è Sample duplicate rows:\")\n",
    "    print(df[df.duplicated(keep=False)].head())\n",
    "else:\n",
    "    print(\"\\n‚úÖ No duplicate rows found!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Outliers (for numeric columns)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Identify numeric columns and check for outliers\n",
    "print(\"=\" * 60)\n",
    "print(\"OUTLIER ANALYSIS (Numeric Columns)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nNumeric columns found: {numeric_cols}\")\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    for col in numeric_cols[:5]:  # Limit to first 5 numeric columns\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outlier_pct = len(outliers) / len(df) * 100\n",
    "        \n",
    "        print(f\"\\nüìä {col}:\")\n",
    "        print(f\"   Range: {df[col].min():.2f} to {df[col].max():.2f}\")\n",
    "        print(f\"   Mean: {df[col].mean():.2f}, Median: {df[col].median():.2f}\")\n",
    "        print(f\"   Outliers: {len(outliers):,} ({outlier_pct:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo numeric columns found for outlier analysis.\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Data Quality Summary Visualization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a visual summary\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Missing values heatmap\n",
    "ax1 = axes[0]\n",
    "missing_matrix = df.isnull().sum().values.reshape(1, -1)\n",
    "sns.heatmap(missing_matrix, annot=True, fmt='d', cmap='YlOrRd', \n",
    "            xticklabels=df.columns, yticklabels=['Missing'], ax=ax1, cbar=False)\n",
    "ax1.set_title('Missing Values by Column', fontsize=12)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Data types distribution\n",
    "ax2 = axes[1]\n",
    "dtype_counts = df.dtypes.astype(str).value_counts()\n",
    "dtype_counts.plot(kind='bar', ax=ax2, color=['steelblue', 'coral', 'green', 'purple'][:len(dtype_counts)])\n",
    "ax2.set_title('Column Data Types', fontsize=12)\n",
    "ax2.set_xlabel('Data Type')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Data Quality Assessment Complete!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Part 4: Find Something Interesting! (5 points)\n",
    "\n",
    "Now it's YOUR turn to explore. Find something interesting, surprising, or useful in your data.\n",
    "\n",
    "**Ideas to explore:**\n",
    "- What's the distribution of a key variable?\n",
    "- Are there any unexpected patterns?\n",
    "- What correlations exist between variables?\n",
    "- What's the most/least common category?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR EXPLORATION CODE HERE\n",
    "# Example: Distribution of a text column's length\n",
    "\n",
    "# Find text columns\n",
    "text_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "if len(text_cols) > 0:\n",
    "    text_col = text_cols[0]  # Use first text column\n",
    "    df['text_length'] = df[text_col].astype(str).str.len()\n",
    "    \n",
    "    print(f\"üìä Text Length Analysis for '{text_col}':\")\n",
    "    print(f\"   Shortest: {df['text_length'].min()} characters\")\n",
    "    print(f\"   Longest: {df['text_length'].max()} characters\")\n",
    "    print(f\"   Average: {df['text_length'].mean():.0f} characters\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.hist(df['text_length'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Text Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Distribution of Text Length in {text_col}')\n",
    "    plt.axvline(df['text_length'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"text_length\"].mean():.0f}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ADD YOUR OWN INTERESTING FINDING HERE!\n",
    "# What pattern, insight, or surprise did you discover?\n",
    "\n",
    "# Example template:\n",
    "print(\"üîç MY INTERESTING FINDING:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"Describe what you found here...\n",
    "\n",
    "- What did you discover?\n",
    "- Why is it interesting or surprising?\n",
    "- What business question could this help answer?\n",
    "\"\"\")\n",
    "\n",
    "# Your analysis code below:\n",
    "# ..."
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, verify you have completed:\n",
    "\n",
    "| Item | Points | Done? |\n",
    "|------|--------|-------|\n",
    "| Part 1: Environment setup works | 3 | ‚òê |\n",
    "| Part 2: Data loaded successfully | 4 | ‚òê |\n",
    "| Part 3: Data quality assessment (missing, duplicates, outliers) | 8 | ‚òê |\n",
    "| Part 4: Found something interesting with explanation | 5 | ‚òê |\n",
    "| **Total** | **20** | |\n",
    "\n",
    "---\n",
    "\n",
    "## How to Submit\n",
    "\n",
    "1. **Run all cells** (Runtime ‚Üí Run all)\n",
    "2. **Save the notebook** (File ‚Üí Save)\n",
    "3. **Download as .ipynb** (File ‚Üí Download ‚Üí Download .ipynb)\n",
    "4. **Upload to Canvas** under HW1 assignment\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [HuggingFace Datasets Documentation](https://huggingface.co/docs/datasets/)\n",
    "- [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "- [Data Quality Best Practices](https://www.ibm.com/topics/data-quality)"
   ],
   "metadata": {}
  }
 ]
}