{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî§ Homework 4: Word Embeddings with Word2Vec\n",
    "**MIS 769 - Big Data Analytics for Business | Spring 2026**\n",
    "\n",
    "**Points:** 20 | **Due:** Sunday, February 22, 2026 @ 11pm Pacific\n",
    "\n",
    "**Author:** Richard Young, Ph.D. | UNLV Lee Business School\n",
    "\n",
    "**Compute:** CPU (free tier)\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. How words become numbers (and why it matters)\n",
    "2. Train your own Word2Vec model\n",
    "3. Document embedding successes AND failures\n",
    "4. Create business-relevant analogies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Loading (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim datasets scikit-learn matplotlib -q\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import random\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "random.seed(769)\n",
    "np.random.seed(769)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Need lots of text for good embeddings\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
    "texts = dataset['text']\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(texts):,} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Prepare Text for Word2Vec (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_word2vec(text):\n",
    "    \"\"\"Clean and tokenize text for Word2Vec.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    return tokens\n",
    "\n",
    "print(\"Preprocessing texts...\")\n",
    "corpus = [preprocess_for_word2vec(text) for text in texts]\n",
    "corpus = [doc for doc in corpus if len(doc) > 5]\n",
    "\n",
    "print(f\"‚úÖ Prepared {len(corpus):,} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train Word2Vec Model (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Training Word2Vec model (1-2 minutes)...\")\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=10,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    epochs=15,\n",
    "    seed=769\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model trained!\")\n",
    "print(f\"   Vocabulary: {len(model.wv):,} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Explore Word Similarities (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [\"good\", \"bad\", \"movie\", \"actor\", \"director\"]\n",
    "\n",
    "print(\"üìä WORD SIMILARITIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for word in test_words:\n",
    "    if word in model.wv:\n",
    "        similar = model.wv.most_similar(word, topn=5)\n",
    "        print(f\"\\n'{word}' is similar to:\")\n",
    "        for similar_word, score in similar:\n",
    "            print(f\"   {similar_word:15} {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Document Failures (4 points)\n",
    "\n",
    "Embeddings aren't magic - find where they fail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antonym problem: antonyms often appear similar!\n",
    "print(\"üîç ANTONYM TEST\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "antonym_pairs = [\n",
    "    (\"good\", \"bad\"),\n",
    "    (\"love\", \"hate\"),\n",
    "    (\"best\", \"worst\"),\n",
    "]\n",
    "\n",
    "for word1, word2 in antonym_pairs:\n",
    "    if word1 in model.wv and word2 in model.wv:\n",
    "        sim = model.wv.similarity(word1, word2)\n",
    "        problem = \"‚ö†Ô∏è TOO SIMILAR!\" if sim > 0.3 else \"‚úì OK\"\n",
    "        print(f\"{word1:10} ‚Üî {word2:10} : {sim:.3f} {problem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR FAILURE HUNT: Find at least 1 case where embeddings give wrong results\n",
    "# Try words you expect to be similar but aren't, or different but are similar\n",
    "\n",
    "# YOUR CODE HERE:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Business Analogies (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analogy(word1, word2, word3, model):\n",
    "    \"\"\"Test: word1 - word2 + word3 = ?\"\"\"\n",
    "    try:\n",
    "        result = model.wv.most_similar(\n",
    "            positive=[word1, word3],\n",
    "            negative=[word2],\n",
    "            topn=3\n",
    "        )\n",
    "        return result\n",
    "    except KeyError as e:\n",
    "        return f\"Word not in vocabulary: {e}\"\n",
    "\n",
    "# Test some analogies\n",
    "analogies = [\n",
    "    (\"good\", \"better\", \"bad\"),  # bad + (better - good) = worse?\n",
    "]\n",
    "\n",
    "for word1, word2, word3 in analogies:\n",
    "    result = test_analogy(word1, word2, word3, model)\n",
    "    print(f\"{word3} - {word2} + {word1} = ?\")\n",
    "    if isinstance(result, list):\n",
    "        for word, score in result:\n",
    "            print(f\"   ‚Üí {word} ({score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANALOGIES: Create at least 3 business-relevant analogies\n",
    "my_analogies = [\n",
    "    # (\"word1\", \"word2\", \"word3\"),\n",
    "]\n",
    "\n",
    "for word1, word2, word3 in my_analogies:\n",
    "    result = test_analogy(word1, word2, word3, model)\n",
    "    print(f\"\\n{word3} - {word2} + {word1} = ?\")\n",
    "    if isinstance(result, list):\n",
    "        for word, score in result:\n",
    "            print(f\"   ‚Üí {word} ({score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Compare to Pre-trained (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• Loading pre-trained GloVe...\")\n",
    "glove = api.load(\"glove-twitter-50\")\n",
    "print(f\"‚úÖ Loaded {len(glove):,} words\")\n",
    "\n",
    "# Compare\n",
    "compare_word = \"good\"\n",
    "print(f\"\\nSimilar to '{compare_word}':\")\n",
    "print(f\"Your model:  {[w for w, _ in model.wv.most_similar(compare_word, topn=5)]}\")\n",
    "print(f\"Pre-trained: {[w for w, _ in glove.most_similar(compare_word, topn=5)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Questions to Answer\n",
    "\n",
    "**Q1:** Which similarities made sense? Which surprised you?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "**Q2:** Document an embedding failure you found.\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "**Q3:** Which analogies worked? Why do some fail?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "**Q4:** How do your embeddings differ from pre-trained?\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "| Item | Points | Done? |\n",
    "|------|--------|-------|\n",
    "| Parts 1-2: Data loaded and preprocessed | 3 | ‚òê |\n",
    "| Part 3: Word2Vec trained | 4 | ‚òê |\n",
    "| Part 4: Similarities explored | 4 | ‚òê |\n",
    "| Part 5: 1+ embedding failure documented | 4 | ‚òê |\n",
    "| Part 6: 3 business analogies | 3 | ‚òê |\n",
    "| Part 7: Pre-trained comparison | 2 | ‚òê |\n",
    "| **Total** | **20** | |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
