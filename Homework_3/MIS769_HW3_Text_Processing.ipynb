{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù Homework 3: Text Processing Fundamentals\n",
    "**MIS 769 - Big Data Analytics for Business | Spring 2026**\n",
    "\n",
    "**Points:** 20 | **Due:** Sunday, February 15, 2026 @ 11pm Pacific\n",
    "\n",
    "**Author:** Richard Young, Ph.D. | UNLV Lee Business School\n",
    "\n",
    "**Compute:** CPU (free tier)\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Install and use NLP libraries (spaCy, NLTK)\n",
    "2. Understand WHY we preprocess text (not just how)\n",
    "3. Create domain-specific stopwords for YOUR data\n",
    "4. Identify cases where cleaning HURTS your analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install spacy nltk datasets wordcloud -q\n",
    "!python -m spacy download en_core_web_sm -q\n",
    "\n",
    "print(\"‚úÖ Libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# spaCy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get standard English stopwords\n",
    "STANDARD_STOPWORDS = set(stopwords.words('english'))\n",
    "print(f\"‚úÖ Loaded {len(STANDARD_STOPWORDS)} standard English stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load Your Data (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"nvidia/HelpSteer2\", split=\"train\")\n",
    "df = dataset.to_pandas()\n",
    "text_column = \"response\"  # Adjust based on your dataset\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df):,} records\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Standard Stopword Removal (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stopwords_set):\n",
    "    \"\"\"Remove stopwords from text.\"\"\"\n",
    "    words = str(text).lower().split()\n",
    "    filtered = [w for w in words if w not in stopwords_set]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "# Apply to dataset\n",
    "df['text_original'] = df[text_column].astype(str)\n",
    "df['text_cleaned'] = df['text_original'].apply(\n",
    "    lambda x: remove_stopwords(x, STANDARD_STOPWORDS)\n",
    ")\n",
    "\n",
    "# Calculate statistics\n",
    "df['word_count_original'] = df['text_original'].str.split().str.len()\n",
    "df['word_count_cleaned'] = df['text_cleaned'].str.split().str.len()\n",
    "df['pct_removed'] = ((df['word_count_original'] - df['word_count_cleaned']) / df['word_count_original'] * 100).round(1)\n",
    "\n",
    "print(\"üìä STOPWORD REMOVAL IMPACT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Average reduction: {df['pct_removed'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Create Domain-Specific Stopwords (5 points)\n",
    "\n",
    "Review the most common words and identify domain-specific stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word frequencies\n",
    "all_words = ' '.join(df['text_cleaned']).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "print(\"üìä TOP 30 MOST COMMON WORDS (after standard cleaning)\")\n",
    "print(\"-\" * 50)\n",
    "for i, (word, count) in enumerate(word_freq.most_common(30), 1):\n",
    "    print(f\"{i:2}. {word:15} {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TASK: Add domain-specific stopwords based on the list above\n",
    "DOMAIN_STOPWORDS = {\n",
    "    # Add at least 10 domain-specific stopwords with justification\n",
    "    # Example:\n",
    "    # 'example',  # appears frequently but adds no meaning\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Created {len(DOMAIN_STOPWORDS)} domain-specific stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: The Negation Problem (5 points)\n",
    "\n",
    "Standard stopwords include negation words (not, no, never) that can change meaning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negation words in standard stopwords\n",
    "negation_words = {'not', 'no', 'never', 'neither', 'nobody', 'none', \"n't\", 'nor'}\n",
    "negations_in_stopwords = negation_words & STANDARD_STOPWORDS\n",
    "\n",
    "print(\"‚ö†Ô∏è DANGER: These negation words are in standard stopwords:\")\n",
    "print(f\"   {negations_in_stopwords}\")\n",
    "\n",
    "# Example of meaning change\n",
    "examples = [\n",
    "    \"This product is not good at all\",\n",
    "    \"I would not recommend this\",\n",
    "]\n",
    "\n",
    "print(\"\\nüìù NEGATION REMOVAL EXAMPLES\")\n",
    "for text in examples:\n",
    "    cleaned = remove_stopwords(text, STANDARD_STOPWORDS)\n",
    "    print(f\"\\nOriginal: {text}\")\n",
    "    print(f\"Cleaned:  {cleaned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SMART stopwords that preserve negations\n",
    "COMBINED_STOPWORDS = STANDARD_STOPWORDS | DOMAIN_STOPWORDS\n",
    "SMART_STOPWORDS = COMBINED_STOPWORDS - negation_words\n",
    "\n",
    "print(f\"‚úÖ Smart stopwords: {len(SMART_STOPWORDS)} (preserves negation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualization (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from wordcloud import WordCloud\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Original\ntext_orig = ' '.join(df['text_original'].sample(500, random_state=103))\nwc1 = WordCloud(width=400, height=300, background_color='white', max_words=50)\nwc1.generate(text_orig)\naxes[0].imshow(wc1)\naxes[0].set_title('Original Text')\naxes[0].axis('off')\n\n# Cleaned\ntext_clean = ' '.join(df['text_cleaned'].sample(500, random_state=103))\nwc2 = WordCloud(width=400, height=300, background_color='white', max_words=50)\nwc2.generate(text_clean)\naxes[1].imshow(wc2)\naxes[1].set_title('After Stopword Removal')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Questions to Answer\n",
    "\n",
    "**Q1:** Which removed stopwords might carry meaning in your domain?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "**Q2:** Why did you choose each domain-specific stopword?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "**Q3:** When should you preserve vs. remove negations?\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "| Item | Points | Done? |\n",
    "|------|--------|-------|\n",
    "| Part 1-2: Setup and data loaded | 3 | ‚òê |\n",
    "| Part 3: Standard stopword analysis | 4 | ‚òê |\n",
    "| Part 4: 10+ domain stopwords with justification | 5 | ‚òê |\n",
    "| Part 5: Negation analysis | 5 | ‚òê |\n",
    "| Part 6: Word cloud visualization | 3 | ‚òê |\n",
    "| **Total** | **20** | |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}