{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "autoregressive.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive LLM Demo\n",
    "## MIS 769 - Advanced Big Data Analytics\n",
    "### Prof. Richard Young (ryoung@unlv.edu)\n",
    "\n",
    "This notebook demonstrates how language models generate text one token at a time (autoregressive generation).\n",
    "\n",
    "Colab notes:\n",
    "- Works on GPU when available (faster)\n",
    "- Falls back to CPU automatically\n",
    "- Installs missing dependencies in-notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab/Jupyter dependency setup\n",
    "import importlib\n",
    "import importlib.util\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_IMPLICIT_TOKEN\"] = \"1\"\n",
    "logging.getLogger(\"huggingface_hub.utils._http\").setLevel(logging.ERROR)\n",
    "\n",
    "needs_install = False\n",
    "for pkg in [\"transformers\", \"accelerate\", \"sentencepiece\"]:\n",
    "    if importlib.util.find_spec(pkg) is None:\n",
    "        needs_install = True\n",
    "\n",
    "if not needs_install:\n",
    "    try:\n",
    "        transformers = importlib.import_module(\"transformers\")\n",
    "        from packaging import version\n",
    "        if version.parse(transformers.__version__) < version.parse(\"4.45.0\"):\n",
    "            needs_install = True\n",
    "            print(f\"Upgrading transformers from {transformers.__version__} to >=4.45.0\")\n",
    "    except Exception:\n",
    "        needs_install = True\n",
    "\n",
    "if needs_install:\n",
    "    subprocess.check_call([\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"pip\",\n",
    "        \"install\",\n",
    "        \"-q\",\n",
    "        \"transformers>=4.45.0\",\n",
    "        \"accelerate>=0.33.0\",\n",
    "        \"sentencepiece\",\n",
    "    ])\n",
    "    print(\"Dependencies installed/updated.\")\n",
    "else:\n",
    "    print(\"Dependencies already installed and version-compatible.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Primary model for class demo, with a small fallback if loading fails\n",
    "PRIMARY_MODEL = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "PRIMARY_REVISION = \"f39ac1d28e925b323eae81227eaba4464caced4e\"\n",
    "FALLBACK_MODEL = \"distilgpt2\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer = None\n",
    "_model = None\n",
    "_loaded_model_id = None\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load the demo model once and reuse it across runs.\"\"\"\n",
    "    global _tokenizer, _model, _loaded_model_id\n",
    "\n",
    "    if _tokenizer is not None and _model is not None:\n",
    "        return _tokenizer, _model, _loaded_model_id\n",
    "\n",
    "    # Use native Transformers implementation for Phi-3 to avoid remote-code rope_scaling issues.\n",
    "    model_candidates = [\n",
    "        (PRIMARY_MODEL, PRIMARY_REVISION, False),\n",
    "        (FALLBACK_MODEL, None, False),\n",
    "    ]\n",
    "\n",
    "    last_error = None\n",
    "    for model_id, revision, trust_remote_code in model_candidates:\n",
    "        try:\n",
    "            tokenizer_kwargs = {\n",
    "                \"token\": False,\n",
    "            }\n",
    "            if trust_remote_code:\n",
    "                tokenizer_kwargs[\"trust_remote_code\"] = True\n",
    "            if revision is not None:\n",
    "                tokenizer_kwargs[\"revision\"] = revision\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id, **tokenizer_kwargs)\n",
    "\n",
    "            model_kwargs = {\n",
    "                \"token\": False,\n",
    "                \"dtype\": torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "                \"low_cpu_mem_usage\": True,\n",
    "            }\n",
    "            if trust_remote_code:\n",
    "                model_kwargs[\"trust_remote_code\"] = True\n",
    "            if revision is not None:\n",
    "                model_kwargs[\"revision\"] = revision\n",
    "            if model_id == PRIMARY_MODEL:\n",
    "                model_kwargs[\"attn_implementation\"] = \"eager\"\n",
    "            if DEVICE == \"cuda\":\n",
    "                model_kwargs[\"device_map\"] = \"auto\"\n",
    "\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
    "\n",
    "            if DEVICE == \"cpu\":\n",
    "                model = model.to(\"cpu\")\n",
    "\n",
    "            if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            _tokenizer, _model, _loaded_model_id = tokenizer, model, model_id\n",
    "            print(f\"Loaded model: {model_id}\")\n",
    "            return _tokenizer, _model, _loaded_model_id\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            print(f\"Could not load {model_id}: {e}\")\n",
    "\n",
    "    raise RuntimeError(\n",
    "        \"Failed to load models from Hugging Face. \"\n",
    "        \"Check Colab internet access and rerun the setup cell.\"\n",
    "    ) from last_error\n",
    "\n",
    "\n",
    "def _build_input_ids(prompt, tokenizer, model_device, model_id, use_chat_template=True):\n",
    "    \"\"\"Use chat template for Phi-3 instruct models, plain tokenization otherwise.\"\"\"\n",
    "    if use_chat_template and model_id == PRIMARY_MODEL and hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        templated = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return templated[\"input_ids\"].to(model_device), True\n",
    "\n",
    "    return tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model_device), False\n",
    "\n",
    "\n",
    "def show_autoregressive_generation(\n",
    "    prompt,\n",
    "    max_tokens=15,\n",
    "    temperature=0.7,\n",
    "    delay=0.3,\n",
    "    use_chat_template=True,\n",
    "    stop_on_sentence_end=False,\n",
    "    seed=None,\n",
    "):\n",
    "    \"\"\"Generate one token at a time to visualize autoregressive decoding.\"\"\"\n",
    "    tokenizer, model, model_id = load_model()\n",
    "\n",
    "    model_device = next(model.parameters()).device\n",
    "    input_ids, used_chat_template = _build_input_ids(\n",
    "        prompt,\n",
    "        tokenizer,\n",
    "        model_device,\n",
    "        model_id,\n",
    "        use_chat_template=use_chat_template,\n",
    "    )\n",
    "\n",
    "    current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    print(f\"\\nModel: {model_id}\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Seed: {seed if seed is not None else 'random'}\")\n",
    "    print(f\"Chat template: {'on' if used_chat_template else 'off'}\")\n",
    "    print(\"\\nGenerating token by token...\")\n",
    "\n",
    "    for step in range(max_tokens):\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        new_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if new_text.startswith(current_text):\n",
    "            new_token = new_text[len(current_text):]\n",
    "        else:\n",
    "            new_token = new_text\n",
    "\n",
    "        print(f\"\\nStep {step + 1}:\")\n",
    "        print(f\"New token: {new_token!r}\")\n",
    "        print(f\"Text so far: {new_text}\")\n",
    "\n",
    "        input_ids = outputs\n",
    "        current_text = new_text\n",
    "\n",
    "        if delay:\n",
    "            time.sleep(delay)\n",
    "\n",
    "        if stop_on_sentence_end and new_text.endswith((\".\", \"!\", \"?\")) and step > 5:\n",
    "            break\n",
    "\n",
    "    return new_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classroom demo: explicit token-by-token generation (Step 1..Step N)\n",
    "prompt = \"Some people like cats but I am more of\"\n",
    "\n",
    "generated_text = show_autoregressive_generation(\n",
    "    prompt,\n",
    "    max_tokens=10,             # Show exactly 10 prediction steps\n",
    "    temperature=0.7,\n",
    "    delay=0.4,\n",
    "    use_chat_template=True,\n",
    "    stop_on_sentence_end=False,\n",
    "    seed=103,                  # Fixed seed for repeatable classroom demos\n",
    ")\n",
    "\n",
    "print(\"\\nFinal output:\", generated_text)\n",
    "\n"
   ]
  }
 ]
}