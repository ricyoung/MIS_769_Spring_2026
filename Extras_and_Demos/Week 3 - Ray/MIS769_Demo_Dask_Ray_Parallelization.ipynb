{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Computing with Dask and Ray\n",
    "\n",
    "**MIS 769 - Advanced Data Analytics**\n",
    "\n",
    "This notebook demonstrates how to parallelize Python code using two popular frameworks:\n",
    "- **Dask**: Flexible parallel computing library for analytics\n",
    "- **Ray**: General-purpose distributed computing framework\n",
    "\n",
    "Both tools allow you to scale from a single laptop to a cluster of machines.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q dask[complete] ray[default] bokeh datasets hf_xet\n\n# Install htop for CPU monitoring (run 'htop' in Terminal to watch cores)\n!apt update -qq && apt install -y htop -qq\n\n# Enable fast Hugging Face downloads\nimport os\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nprint(\"‚úì All packages installed. Open Terminal and run 'htop' to monitor CPU cores.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For timing comparisons\n",
    "def timer(func):\n",
    "    \"\"\"Decorator to time function execution\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"  Execution time: {end - start:.3f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# The \"30-Second Demo\" - Why Parallelization Matters\n\n**Before we dive into Dask and Ray, let's see WHY you need these tools.**\n\nWe'll create a dataset large enough that Pandas takes ~30 seconds. Then we'll see how Dask handles the same operation.\n\n**Pro tip:** Open your system's Activity Monitor (Mac), Task Manager (Windows), or `htop` (Linux) before running these cells. Watch what happens to your CPU cores!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load NYC Taxi dataset - using Hugging Face\nfrom datasets import load_dataset\nimport pandas as pd\n\nprint(\"Loading NYC Taxi dataset from Hugging Face...\")\nprint(\"This is a large dataset - may take a few minutes...\\n\")\n\n# Load first 20 million rows for the demo\ndataset = load_dataset(\n    \"JosephFeig/NYC-Taxi\", \n    split=\"train[:20000000]\"\n)\n\ntaxi_df = dataset.to_pandas()\n\nprint(f\"\\nDataset shape: {taxi_df.shape}\")\nprint(f\"Memory usage: {taxi_df.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\nprint(f\"\\nColumns: {list(taxi_df.columns)}\")\ntaxi_df.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CONFIGURATION - Adjust these to see different behaviors\n# ============================================================\nN_CHUNKS = 32           # More chunks = longer sequential time\nROLLING_ITERATIONS = 20  # More iterations = heavier computation per chunk\n\n# ============================================================\n# PANDAS - Sequential Processing\n# ============================================================\n# Process data in chunks sequentially (simulating real workload)\n\nprint(\"=\" * 60)\nprint(\"PANDAS/SEQUENTIAL - One chunk at a time\")\nprint(\"=\" * 60)\nprint(f\"\\nProcessing {len(taxi_df):,} rows in {N_CHUNKS} chunks sequentially...\")\nprint(f\"Rolling iterations per chunk: {ROLLING_ITERATIONS}\\n\")\n\nimport numpy as np\n\ndef heavy_computation(df_chunk):\n    \"\"\"Simulate heavy per-row computation\"\"\"\n    result = {\n        'total_fares': df_chunk['fare_amount'].sum(),\n        'avg_fare': df_chunk['fare_amount'].mean(),\n        'std_fare': df_chunk['fare_amount'].std(),\n        'avg_distance': np.sqrt(\n            (df_chunk['dropoff_latitude'] - df_chunk['pickup_latitude'])**2 +\n            (df_chunk['dropoff_longitude'] - df_chunk['pickup_longitude'])**2\n        ).mean(),\n        'fare_per_passenger': (df_chunk['fare_amount'] / df_chunk['passenger_count'].clip(1)).mean(),\n        'row_count': len(df_chunk)\n    }\n    # Heavy computation - rolling windows are CPU intensive\n    for _ in range(ROLLING_ITERATIONS):\n        _ = df_chunk['fare_amount'].rolling(100, min_periods=1).mean().sum()\n        _ = df_chunk['fare_amount'].rolling(50, min_periods=1).std().sum()\n    return result\n\n# Split into chunks and process sequentially\nchunks = np.array_split(taxi_df, N_CHUNKS)\n\nstart_time = time.time()\n\nsequential_results = []\nfor i, chunk in enumerate(chunks):\n    result = heavy_computation(chunk)\n    sequential_results.append(result)\n    if (i + 1) % 8 == 0:  # Print every 8 chunks\n        print(f\"  Chunks 1-{i+1} of {N_CHUNKS} done...\")\n\npandas_time = time.time() - start_time\nprint(f\"\\n‚úì Sequential execution time: {pandas_time:.2f} seconds\")\n\n# Aggregate results\ntotal_fares = sum(r['total_fares'] for r in sequential_results)\nprint(f\"  Total fares processed: ${total_fares:,.0f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# DASK - Parallel Processing\n# ============================================================\n# Process ALL chunks at the same time!\n\nfrom dask import delayed, compute\n\nprint(\"=\" * 60)\nprint(\"DASK - All chunks in PARALLEL\")\nprint(\"=\" * 60)\nprint(f\"\\nProcessing {len(taxi_df):,} rows in {N_CHUNKS} chunks in parallel...\\n\")\n\n# Same function, wrapped with delayed for parallel execution\n@delayed\ndef heavy_computation_delayed(df_chunk):\n    \"\"\"Same computation, but will run in parallel\"\"\"\n    result = {\n        'total_fares': df_chunk['fare_amount'].sum(),\n        'avg_fare': df_chunk['fare_amount'].mean(),\n        'std_fare': df_chunk['fare_amount'].std(),\n        'avg_distance': np.sqrt(\n            (df_chunk['dropoff_latitude'] - df_chunk['pickup_latitude'])**2 +\n            (df_chunk['dropoff_longitude'] - df_chunk['pickup_longitude'])**2\n        ).mean(),\n        'fare_per_passenger': (df_chunk['fare_amount'] / df_chunk['passenger_count'].clip(1)).mean(),\n        'row_count': len(df_chunk)\n    }\n    # Same heavy computation\n    for _ in range(ROLLING_ITERATIONS):\n        _ = df_chunk['fare_amount'].rolling(100, min_periods=1).mean().sum()\n        _ = df_chunk['fare_amount'].rolling(50, min_periods=1).std().sum()\n    return result\n\nstart_time = time.time()\n\n# Launch ALL chunks in parallel (this is the key difference!)\nparallel_tasks = [heavy_computation_delayed(chunk) for chunk in chunks]\n\n# Execute all at once\ndask_results = compute(*parallel_tasks)\n\ndask_time = time.time() - start_time\n\nprint(f\"‚úì All {N_CHUNKS} chunks completed!\")\nprint(f\"\\n‚úì Dask parallel execution time: {dask_time:.2f} seconds\")\n\n# Aggregate results\ntotal_fares = sum(r['total_fares'] for r in dask_results)\nprint(f\"  Total fares processed: ${total_fares:,.0f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# RAY - Parallel Processing (Alternative to Dask)\n# ============================================================\n# Same task, but using Ray's @ray.remote decorator\n\nimport ray\nray.init(ignore_reinit_error=True)\n\nprint(\"=\" * 60)\nprint(\"RAY - All chunks in PARALLEL\")\nprint(\"=\" * 60)\nprint(f\"\\nProcessing {len(taxi_df):,} rows in {N_CHUNKS} chunks in parallel...\\n\")\n\n# Same function, but with Ray's remote decorator\n@ray.remote\ndef heavy_computation_ray(df_chunk, n_iterations):\n    \"\"\"Same computation, but runs as a Ray task\"\"\"\n    import numpy as np\n    result = {\n        'total_fares': df_chunk['fare_amount'].sum(),\n        'avg_fare': df_chunk['fare_amount'].mean(),\n        'std_fare': df_chunk['fare_amount'].std(),\n        'avg_distance': np.sqrt(\n            (df_chunk['dropoff_latitude'] - df_chunk['pickup_latitude'])**2 +\n            (df_chunk['dropoff_longitude'] - df_chunk['pickup_longitude'])**2\n        ).mean(),\n        'fare_per_passenger': (df_chunk['fare_amount'] / df_chunk['passenger_count'].clip(1)).mean(),\n        'row_count': len(df_chunk)\n    }\n    # Same heavy computation\n    for _ in range(n_iterations):\n        _ = df_chunk['fare_amount'].rolling(100, min_periods=1).mean().sum()\n        _ = df_chunk['fare_amount'].rolling(50, min_periods=1).std().sum()\n    return result\n\nstart_time = time.time()\n\n# Launch ALL chunks in parallel with Ray\nfutures = [heavy_computation_ray.remote(chunk, ROLLING_ITERATIONS) for chunk in chunks]\n\n# Wait for all results\nray_results = ray.get(futures)\n\nray_time = time.time() - start_time\n\nprint(f\"‚úì All {N_CHUNKS} chunks completed!\")\nprint(f\"\\n‚úì Ray parallel execution time: {ray_time:.2f} seconds\")\n\n# Aggregate results\ntotal_fares = sum(r['total_fares'] for r in ray_results)\nprint(f\"  Total fares processed: ${total_fares:,.0f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# RESULTS COMPARISON - Sequential vs Dask vs Ray\n# ============================================================\n\nimport os\nn_cores = os.cpu_count() or 8\n\ndask_speedup = pandas_time / dask_time\nray_speedup = pandas_time / ray_time\nbest_time = min(dask_time, ray_time)\nbest_method = \"Dask\" if dask_time < ray_time else \"Ray\"\nbest_speedup = pandas_time / best_time\n\nprint()\nprint(\"‚îå\" + \"‚îÄ\" * 58 + \"‚îê\")\nprint(\"‚îÇ\" + \"  RESULTS: Sequential vs Parallel\".center(58) + \"‚îÇ\")\nprint(\"‚îú\" + \"‚îÄ\" * 58 + \"‚î§\")\nprint(f\"‚îÇ  Config: {N_CHUNKS} chunks, {ROLLING_ITERATIONS} iterations, {n_cores} cores\".ljust(59) + \"‚îÇ\")\nprint(\"‚îú\" + \"‚îÄ\" * 58 + \"‚î§\")\nprint(f\"‚îÇ  Sequential (for loop):   {pandas_time:>8.2f}s    (baseline)\".ljust(59) + \"‚îÇ\")\nprint(f\"‚îÇ  Dask (@delayed):         {dask_time:>8.2f}s    {dask_speedup:>5.1f}x faster\".ljust(59) + \"‚îÇ\")\nprint(f\"‚îÇ  Ray (@ray.remote):       {ray_time:>8.2f}s    {ray_speedup:>5.1f}x faster\".ljust(59) + \"‚îÇ\")\nprint(\"‚îú\" + \"‚îÄ\" * 58 + \"‚î§\")\nprint(f\"‚îÇ  üèÜ Winner: {best_method} at {best_speedup:.1f}x speedup!\".ljust(59) + \"‚îÇ\")\nprint(\"‚îî\" + \"‚îÄ\" * 58 + \"‚îò\")\n\n# Quick visual\nbar_width = 40\nseq_bar = bar_width\ndask_bar = int(bar_width * dask_time / pandas_time)\nray_bar = int(bar_width * ray_time / pandas_time)\n\nprint(f\"\\n  Sequential: {'‚ñà' * seq_bar} {pandas_time:.1f}s\")\nprint(f\"  Dask:       {'‚ñà' * dask_bar}{' ' * (seq_bar - dask_bar)} {dask_time:.1f}s\")\nprint(f\"  Ray:        {'‚ñà' * ray_bar}{' ' * (seq_bar - ray_bar)} {ray_time:.1f}s\")\n\nprint(\"\\n  ‚Üì Scroll down for detailed Dask & Ray tutorials ‚Üì\")\nprint(\"  ‚Üì Or jump to the FINAL SUMMARY at the end ‚Üì\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Dask\n",
    "\n",
    "Dask is a flexible parallel computing library that integrates seamlessly with the Python ecosystem. It provides:\n",
    "\n",
    "- **Dask Arrays**: Parallel NumPy arrays\n",
    "- **Dask DataFrames**: Parallel Pandas DataFrames\n",
    "- **Dask Delayed**: Parallelize custom Python functions\n",
    "- **Dask Bag**: Parallel lists for semi-structured data\n",
    "\n",
    "Key concept: Dask builds a **task graph** of operations and executes them in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "from dask.distributed import Client\n",
    "\n",
    "print(f\"Dask version: {dask.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dask Distributed Client\n",
    "\n",
    "The Dask distributed scheduler provides a dashboard for monitoring tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a local Dask cluster\n",
    "client = Client(n_workers=4, threads_per_worker=2, memory_limit='2GB')\n",
    "print(client)\n",
    "print(f\"\\nDashboard link: {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dask Arrays (Parallel NumPy)\n",
    "\n",
    "Dask arrays work like NumPy arrays but are split into chunks that can be processed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large array with NumPy (standard approach)\n",
    "print(\"NumPy (sequential):\")\n",
    "\n",
    "@timer\n",
    "def numpy_computation():\n",
    "    x = np.random.random((20000, 20000))\n",
    "    result = (x + x.T).mean()\n",
    "    return result\n",
    "\n",
    "numpy_result = numpy_computation()\n",
    "print(f\"  Result: {numpy_result:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same computation with Dask (parallel)\n",
    "print(\"Dask Array (parallel):\")\n",
    "\n",
    "@timer\n",
    "def dask_computation():\n",
    "    # Create a Dask array with chunks of 5000x5000\n",
    "    x = da.random.random((20000, 20000), chunks=(5000, 5000))\n",
    "    result = (x + x.T).mean()\n",
    "    # .compute() triggers actual execution\n",
    "    return result.compute()\n",
    "\n",
    "dask_result = dask_computation()\n",
    "print(f\"  Result: {dask_result:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the task graph (for a smaller example)\n",
    "x = da.random.random((1000, 1000), chunks=(500, 500))\n",
    "y = (x + x.T).mean()\n",
    "\n",
    "print(\"Task graph for: (x + x.T).mean()\")\n",
    "print(f\"Number of tasks: {len(y.__dask_graph__())}\")\n",
    "\n",
    "# Uncomment to visualize (requires graphviz)\n",
    "# y.visualize(filename='dask_graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Dask DataFrames (Parallel Pandas)\n",
    "\n",
    "Dask DataFrames are partitioned Pandas DataFrames that can be larger than memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset\n",
    "n_rows = 5_000_000\n",
    "\n",
    "print(f\"Creating dataset with {n_rows:,} rows...\")\n",
    "df = pd.DataFrame({\n",
    "    'id': np.arange(n_rows),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], n_rows),\n",
    "    'value1': np.random.randn(n_rows) * 100,\n",
    "    'value2': np.random.randn(n_rows) * 50,\n",
    "    'timestamp': pd.date_range('2020-01-01', periods=n_rows, freq='s')\n",
    "})\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas groupby (sequential)\n",
    "print(\"Pandas GroupBy (sequential):\")\n",
    "\n",
    "@timer\n",
    "def pandas_groupby():\n",
    "    return df.groupby('category').agg({\n",
    "        'value1': ['mean', 'std', 'min', 'max'],\n",
    "        'value2': ['mean', 'std', 'min', 'max']\n",
    "    })\n",
    "\n",
    "pandas_result = pandas_groupby()\n",
    "print(pandas_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Dask DataFrame\n",
    "ddf = dd.from_pandas(df, npartitions=8)\n",
    "print(f\"Dask DataFrame: {ddf.npartitions} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask groupby (parallel)\n",
    "print(\"Dask GroupBy (parallel):\")\n",
    "\n",
    "@timer\n",
    "def dask_groupby():\n",
    "    return ddf.groupby('category').agg({\n",
    "        'value1': ['mean', 'std', 'min', 'max'],\n",
    "        'value2': ['mean', 'std', 'min', 'max']\n",
    "    }).compute()\n",
    "\n",
    "dask_result = dask_groupby()\n",
    "print(dask_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Dask Delayed (Custom Parallelization)\n",
    "\n",
    "`dask.delayed` lets you parallelize any Python function by building a task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a slow computation\n",
    "def slow_square(x):\n",
    "    \"\"\"Simulates a slow computation\"\"\"\n",
    "    time.sleep(1)  # Simulate work\n",
    "    return x ** 2\n",
    "\n",
    "def slow_sum(values):\n",
    "    \"\"\"Simulates aggregation\"\"\"\n",
    "    time.sleep(0.5)\n",
    "    return sum(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential execution\n",
    "print(\"Sequential execution:\")\n",
    "\n",
    "@timer\n",
    "def sequential_computation():\n",
    "    results = []\n",
    "    for i in range(8):\n",
    "        results.append(slow_square(i))\n",
    "    return slow_sum(results)\n",
    "\n",
    "seq_result = sequential_computation()\n",
    "print(f\"  Result: {seq_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel execution with dask.delayed\n",
    "print(\"Parallel execution with Dask Delayed:\")\n",
    "\n",
    "@timer\n",
    "def parallel_computation():\n",
    "    # Wrap functions with delayed\n",
    "    delayed_square = delayed(slow_square)\n",
    "    delayed_sum = delayed(slow_sum)\n",
    "    \n",
    "    # Build task graph (no computation yet)\n",
    "    results = []\n",
    "    for i in range(8):\n",
    "        results.append(delayed_square(i))\n",
    "    \n",
    "    total = delayed_sum(results)\n",
    "    \n",
    "    # Execute in parallel\n",
    "    return total.compute()\n",
    "\n",
    "par_result = parallel_computation()\n",
    "print(f\"  Result: {par_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Dask client\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Ray\n",
    "\n",
    "Ray is a general-purpose distributed computing framework that makes it easy to scale Python applications. Key features:\n",
    "\n",
    "- **Ray Core**: Remote functions and actors\n",
    "- **Ray Data**: Scalable data processing\n",
    "- **Ray Train**: Distributed ML training\n",
    "- **Ray Serve**: Model serving\n",
    "\n",
    "Key concept: Ray uses **tasks** (stateless) and **actors** (stateful) as building blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "print(f\"Ray version: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray\n",
    "ray.init(num_cpus=4, ignore_reinit_error=True)\n",
    "print(f\"Ray initialized with {ray.available_resources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Ray Remote Functions (Tasks)\n",
    "\n",
    "The `@ray.remote` decorator turns a Python function into a distributed task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a remote function\n",
    "@ray.remote\n",
    "def ray_slow_square(x):\n",
    "    \"\"\"Remote version of slow_square\"\"\"\n",
    "    time.sleep(1)\n",
    "    return x ** 2\n",
    "\n",
    "@ray.remote\n",
    "def ray_slow_sum(values):\n",
    "    \"\"\"Remote version of slow_sum\"\"\"\n",
    "    time.sleep(0.5)\n",
    "    return sum(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential execution (for comparison)\n",
    "print(\"Sequential execution:\")\n",
    "\n",
    "@timer\n",
    "def sequential_ray():\n",
    "    results = []\n",
    "    for i in range(8):\n",
    "        results.append(slow_square(i))\n",
    "    return slow_sum(results)\n",
    "\n",
    "seq_result = sequential_ray()\n",
    "print(f\"  Result: {seq_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel execution with Ray\n",
    "print(\"Parallel execution with Ray:\")\n",
    "\n",
    "@timer\n",
    "def parallel_ray():\n",
    "    # Launch tasks (returns futures immediately)\n",
    "    futures = [ray_slow_square.remote(i) for i in range(8)]\n",
    "    \n",
    "    # Get results (blocks until done)\n",
    "    results = ray.get(futures)\n",
    "    \n",
    "    # Sum the results\n",
    "    total_future = ray_slow_sum.remote(results)\n",
    "    return ray.get(total_future)\n",
    "\n",
    "par_result = parallel_ray()\n",
    "print(f\"  Result: {par_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Ray Actors (Stateful Computation)\n",
    "\n",
    "Actors are stateful workers that can maintain state across method calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Counter:\n",
    "    \"\"\"A simple counter actor\"\"\"\n",
    "    def __init__(self, initial_value=0):\n",
    "        self.value = initial_value\n",
    "    \n",
    "    def increment(self, amount=1):\n",
    "        self.value += amount\n",
    "        return self.value\n",
    "    \n",
    "    def get_value(self):\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple counter actors\n",
    "counters = [Counter.remote(i * 10) for i in range(4)]\n",
    "\n",
    "# Increment each counter in parallel\n",
    "futures = [c.increment.remote(5) for c in counters]\n",
    "results = ray.get(futures)\n",
    "print(f\"After increment: {results}\")\n",
    "\n",
    "# Get final values\n",
    "final_futures = [c.get_value.remote() for c in counters]\n",
    "final_values = ray.get(final_futures)\n",
    "print(f\"Final values: {final_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Ray for Data Processing\n",
    "\n",
    "Ray can efficiently parallelize data processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def process_chunk(chunk_data):\n",
    "    \"\"\"Process a chunk of data\"\"\"\n",
    "    # Simulate some processing\n",
    "    result = {\n",
    "        'count': len(chunk_data),\n",
    "        'mean': chunk_data['value1'].mean(),\n",
    "        'std': chunk_data['value1'].std(),\n",
    "        'sum': chunk_data['value1'].sum()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the DataFrame we created earlier\n",
    "print(f\"Processing DataFrame with {len(df):,} rows\")\n",
    "\n",
    "# Sequential processing\n",
    "print(\"\\nSequential processing:\")\n",
    "\n",
    "@timer\n",
    "def sequential_process():\n",
    "    chunks = np.array_split(df, 8)\n",
    "    results = []\n",
    "    for chunk in chunks:\n",
    "        result = {\n",
    "            'count': len(chunk),\n",
    "            'mean': chunk['value1'].mean(),\n",
    "            'std': chunk['value1'].std(),\n",
    "            'sum': chunk['value1'].sum()\n",
    "        }\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "seq_results = sequential_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel processing with Ray\n",
    "print(\"Parallel processing with Ray:\")\n",
    "\n",
    "@timer\n",
    "def parallel_process():\n",
    "    # Split data into chunks\n",
    "    chunks = np.array_split(df, 8)\n",
    "    \n",
    "    # Put chunks in Ray's object store\n",
    "    chunk_refs = [ray.put(chunk) for chunk in chunks]\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    futures = [process_chunk.remote(ref) for ref in chunk_refs]\n",
    "    \n",
    "    # Collect results\n",
    "    return ray.get(futures)\n",
    "\n",
    "par_results = parallel_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "total_count = sum(r['count'] for r in par_results)\n",
    "weighted_mean = sum(r['mean'] * r['count'] for r in par_results) / total_count\n",
    "total_sum = sum(r['sum'] for r in par_results)\n",
    "\n",
    "print(f\"\\nAggregated Results:\")\n",
    "print(f\"  Total rows processed: {total_count:,}\")\n",
    "print(f\"  Weighted mean: {weighted_mean:.4f}\")\n",
    "print(f\"  Total sum: {total_sum:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Ray for Monte Carlo Simulation\n",
    "\n",
    "A practical example: estimating Pi using Monte Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pi_sequential(n_samples):\n",
    "    \"\"\"Estimate Pi using Monte Carlo (sequential)\"\"\"\n",
    "    inside_circle = 0\n",
    "    for _ in range(n_samples):\n",
    "        x, y = np.random.random(), np.random.random()\n",
    "        if x**2 + y**2 <= 1:\n",
    "            inside_circle += 1\n",
    "    return 4 * inside_circle / n_samples\n",
    "\n",
    "@ray.remote\n",
    "def estimate_pi_chunk(n_samples):\n",
    "    \"\"\"Estimate Pi for a chunk of samples (vectorized)\"\"\"\n",
    "    x = np.random.random(n_samples)\n",
    "    y = np.random.random(n_samples)\n",
    "    inside = np.sum(x**2 + y**2 <= 1)\n",
    "    return inside, n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total = 10_000_000\n",
    "n_workers = 8\n",
    "samples_per_worker = n_total // n_workers\n",
    "\n",
    "print(f\"Estimating Pi with {n_total:,} samples\\n\")\n",
    "\n",
    "# Sequential (vectorized for fair comparison)\n",
    "print(\"Sequential (vectorized):\")\n",
    "\n",
    "@timer\n",
    "def sequential_pi():\n",
    "    x = np.random.random(n_total)\n",
    "    y = np.random.random(n_total)\n",
    "    inside = np.sum(x**2 + y**2 <= 1)\n",
    "    return 4 * inside / n_total\n",
    "\n",
    "pi_seq = sequential_pi()\n",
    "print(f\"  Pi estimate: {pi_seq:.6f} (error: {abs(pi_seq - np.pi):.6f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel with Ray\n",
    "print(\"Parallel with Ray:\")\n",
    "\n",
    "@timer\n",
    "def parallel_pi():\n",
    "    futures = [estimate_pi_chunk.remote(samples_per_worker) for _ in range(n_workers)]\n",
    "    results = ray.get(futures)\n",
    "    \n",
    "    total_inside = sum(r[0] for r in results)\n",
    "    total_samples = sum(r[1] for r in results)\n",
    "    return 4 * total_inside / total_samples\n",
    "\n",
    "pi_par = parallel_pi()\n",
    "print(f\"  Pi estimate: {pi_par:.6f} (error: {abs(pi_par - np.pi):.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Comparison and Best Practices\n",
    "\n",
    "## When to Use Each Framework\n",
    "\n",
    "| Feature | Dask | Ray |\n",
    "|---------|------|-----|\n",
    "| **Best for** | Data analytics, ETL | General distributed computing |\n",
    "| **DataFrame support** | Native (Dask DataFrame) | Via Modin or pandas |\n",
    "| **Array support** | Native (Dask Array) | Via NumPy |\n",
    "| **ML Training** | Limited | Ray Train, Ray Tune |\n",
    "| **Stateful computation** | Limited | Native (Actors) |\n",
    "| **Learning curve** | Easy (pandas-like API) | Moderate |\n",
    "| **Task graphs** | Explicit visualization | Implicit |\n",
    "| **Integration** | NumPy, Pandas, Scikit-learn | TensorFlow, PyTorch, Hugging Face |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Dask\n",
    "1. **Choose appropriate chunk sizes**: Too small = overhead, too large = memory issues\n",
    "2. **Use `persist()` for intermediate results** that are reused\n",
    "3. **Avoid eager computation**: Chain operations before calling `.compute()`\n",
    "4. **Monitor the dashboard**: Watch for task distribution and memory usage\n",
    "\n",
    "### Ray\n",
    "1. **Use `ray.put()` for large objects** passed to multiple tasks\n",
    "2. **Batch small tasks**: Ray has overhead per task\n",
    "3. **Use actors for stateful computation**: Counters, caches, models\n",
    "4. **Specify resource requirements**: `@ray.remote(num_cpus=2)` for accurate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Ray\n",
    "ray.shutdown()\n",
    "print(\"Ray shutdown complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "In this notebook, we explored two powerful parallelization frameworks:\n",
    "\n",
    "## Dask\n",
    "- **Dask Arrays**: Drop-in replacement for NumPy with automatic chunking\n",
    "- **Dask DataFrames**: Scale pandas workflows to larger-than-memory data\n",
    "- **Dask Delayed**: Parallelize arbitrary Python functions\n",
    "- Great for data analytics and ETL pipelines\n",
    "\n",
    "## Ray\n",
    "- **Remote Functions**: Turn any function into a distributed task with `@ray.remote`\n",
    "- **Actors**: Stateful distributed objects for complex workflows\n",
    "- **Object Store**: Efficient data sharing between tasks\n",
    "- Great for general-purpose distributed computing and ML\n",
    "\n",
    "## Key Takeaways\n",
    "1. Both frameworks can significantly speed up computations\n",
    "2. Dask is more \"pandas-like\" and easier for data analysts\n",
    "3. Ray is more flexible and better for custom distributed applications\n",
    "4. Both can scale from a laptop to a cluster without code changes\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "- [Dask Documentation](https://docs.dask.org/)\n",
    "- [Ray Documentation](https://docs.ray.io/)\n",
    "- [Dask vs Ray Comparison](https://docs.ray.io/en/latest/ray-more-libs/dask-on-ray.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Final Benchmark Summary\n\n**Recall the results from our \"30-Second Demo\" at the beginning of this notebook.**\n\nRun the cell below to see the final comparison of Sequential vs Dask vs Ray on the NYC Taxi dataset.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# üèÅ FINAL BENCHMARK RESULTS - Complete Summary\n# ============================================================\n\ntry:\n    import os\n    n_cores = os.cpu_count() or 8\n    \n    # Calculate speedups\n    dask_speedup = pandas_time / dask_time\n    ray_speedup = pandas_time / ray_time\n    best_parallel_time = min(dask_time, ray_time)\n    best_speedup = pandas_time / best_parallel_time\n    winner = \"DASK\" if dask_time < ray_time else \"RAY\"\n    \n    # Time saved\n    time_saved_dask = pandas_time - dask_time\n    time_saved_ray = pandas_time - ray_time\n    \n    print()\n    print(\"‚ïî\" + \"‚ïê\" * 63 + \"‚ïó\")\n    print(\"‚ïë\" + \"  üèÅ FINAL BENCHMARK RESULTS - NYC Taxi Dataset\".center(63) + \"‚ïë\")\n    print(\"‚ï†\" + \"‚ïê\" * 63 + \"‚ï£\")\n    print(f\"‚ïë  Dataset:        {len(taxi_df):>15,} rows\".ljust(64) + \"‚ïë\")\n    print(f\"‚ïë  Chunks:         {N_CHUNKS:>15}\".ljust(64) + \"‚ïë\")\n    print(f\"‚ïë  CPU Cores:      {n_cores:>15}\".ljust(64) + \"‚ïë\")\n    print(f\"‚ïë  Iterations:     {ROLLING_ITERATIONS:>15} (rolling windows per chunk)\".ljust(64) + \"‚ïë\")\n    print(\"‚ï†\" + \"‚ïê\" * 63 + \"‚ï£\")\n    print(\"‚ïë\" + \"  METHOD                    TIME         SPEEDUP\".ljust(63) + \"‚ïë\")\n    print(\"‚ï†\" + \"‚ïê\" * 63 + \"‚ï£\")\n    print(f\"‚ïë  Sequential (for loop)   {pandas_time:>8.2f}s        1.0x  (baseline)\".ljust(64) + \"‚ïë\")\n    print(f\"‚ïë  Dask (@delayed)         {dask_time:>8.2f}s       {dask_speedup:>5.1f}x  ‚ö°\".ljust(64) + \"‚ïë\")\n    print(f\"‚ïë  Ray (@ray.remote)       {ray_time:>8.2f}s       {ray_speedup:>5.1f}x  ‚ö°\".ljust(64) + \"‚ïë\")\n    print(\"‚ï†\" + \"‚ïê\" * 63 + \"‚ï£\")\n    print(f\"‚ïë  üèÜ WINNER: {winner} at {best_speedup:.1f}x faster!\".ljust(64) + \"‚ïë\")\n    print(\"‚ïö\" + \"‚ïê\" * 63 + \"‚ïù\")\n    \n    # Visual bar chart\n    print(\"\\n  üìä VISUAL COMPARISON:\")\n    print(\"  \" + \"‚îÄ\" * 55)\n    \n    max_time = pandas_time\n    bar_width = 45\n    \n    seq_bar = int(bar_width * pandas_time / max_time)\n    dask_bar = int(bar_width * dask_time / max_time)\n    ray_bar = int(bar_width * ray_time / max_time)\n    \n    print(f\"  Sequential ‚îÇ{'‚ñà' * seq_bar}‚îÇ {pandas_time:.1f}s\")\n    print(f\"  Dask       ‚îÇ{'‚ñà' * dask_bar}{'‚ñë' * (seq_bar - dask_bar)}‚îÇ {dask_time:.1f}s (-{time_saved_dask:.1f}s)\")\n    print(f\"  Ray        ‚îÇ{'‚ñà' * ray_bar}{'‚ñë' * (seq_bar - ray_bar)}‚îÇ {ray_time:.1f}s (-{time_saved_ray:.1f}s)\")\n    print(\"  \" + \"‚îÄ\" * 55)\n    \n    # What this means\n    print(\"\\n  üí° WHAT THIS MEANS:\")\n    print(\"  \" + \"‚îÄ\" * 55)\n    print(f\"  ‚Ä¢ Sequential processed {N_CHUNKS} chunks ONE AT A TIME\")\n    print(f\"  ‚Ä¢ Dask/Ray processed up to {n_cores} chunks SIMULTANEOUSLY\")\n    print(f\"  ‚Ä¢ You saved {max(time_saved_dask, time_saved_ray):.1f} seconds with parallelization!\")\n    \n    if N_CHUNKS > n_cores:\n        waves = N_CHUNKS // n_cores\n        print(f\"  ‚Ä¢ With {N_CHUNKS} chunks and {n_cores} cores, that's ~{waves} waves of parallel work\")\n    \n    print(\"\\n  üéØ KEY TAKEAWAY:\")\n    print(\"  \" + \"‚îÄ\" * 55)\n    print(\"  Same data. Same computation. Same results.\")\n    print(f\"  But {best_speedup:.0f}x FASTER with parallel processing!\")\n    print()\n    print(\"  This is why tools like Dask and Ray matter for Big Data.\")\n    print()\n\nexcept NameError as e:\n    print(\"‚ö†Ô∏è  Benchmark variables not found!\")\n    print(\"   Please run the '30-Second Demo' cells first (cells 5-9)\")\n    print(f\"   Missing: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}