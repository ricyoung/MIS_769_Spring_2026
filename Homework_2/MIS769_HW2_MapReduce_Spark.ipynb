{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# ‚ö° Homework 2: MapReduce Concepts & Spark Fundamentals\n**MIS 769 - Big Data Analytics for Business | Spring 2026**\n\n**Points:** 20 | **Due:** Sunday, February 8, 2026 @ 11pm Pacific\n\n**Author:** Richard Young, Ph.D. | UNLV Lee Business School\n\n**Compute:** CPU (free tier)\n\n---\n\n## What You'll Learn\n\n1. Set up Apache Spark on Google Colab\n2. Understand how Spark partitions data for parallel processing\n3. **Measure and compare** processing performance with different configurations\n4. Apply K-Means clustering and interpret business results\n5. **Draw your own diagram** explaining distributed computing\n\n---\n\n## The Big Picture\n\nWhen data gets too big for one computer, we split the work across many computers. **Spark** is the industry-standard tool for this.\n\n```\nYOUR LAPTOP                           SPARK CLUSTER\n+----------------------+            +---------------------------+\n|                      |            |     Driver Program        |\n|   1 million rows     |            |         +----+            |\n|   Takes: 10 minutes  |   ------>  |         |    |            |\n|                      |            |    +----+----+----+       |\n+----------------------+            |    |    |    |    |       |\n                                    |   W1   W2   W3   W4       |\n                                    |  250K 250K 250K 250K      |\n                                    |  each worker in parallel  |\n                                    +---------------------------+\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Part 1: Spark Environment Setup (3 points)\n",
    "\n",
    "Let's install and configure Apache Spark on Google Colab."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Step 1: Install Java and PySpark\n# IMPORTANT: Run this cell first, then wait for it to complete before running the next cell\n\nimport os\nimport subprocess\n\n# Install Java (try Java 11 first, fallback to Java 8)\nprint(\"Installing Java...\")\nresult = subprocess.run(\"apt-get update && apt-get install -y openjdk-11-jdk-headless\",\n                       shell=True, capture_output=True, text=True)\n\n# Find Java installation path\njava_paths = [\n    \"/usr/lib/jvm/java-11-openjdk-amd64\",\n    \"/usr/lib/jvm/java-8-openjdk-amd64\",\n    \"/usr/lib/jvm/default-java\"\n]\n\njava_home = None\nfor path in java_paths:\n    if os.path.exists(path):\n        java_home = path\n        break\n\nif java_home:\n    os.environ[\"JAVA_HOME\"] = java_home\n    os.environ[\"PATH\"] = f\"{java_home}/bin:\" + os.environ.get(\"PATH\", \"\")\n    print(f\"‚úÖ JAVA_HOME set to: {java_home}\")\nelse:\n    print(\"‚ö†Ô∏è Java not found in expected paths\")\n\n# Verify Java works\nresult = subprocess.run(\"java -version\", shell=True, capture_output=True, text=True)\nprint(f\"Java version: {result.stderr.split(chr(10))[0]}\")\n\n# Install PySpark\n!pip install pyspark --quiet\nprint(\"‚úÖ PySpark installed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 2: Create a Spark Session with 2 cores\n# Make sure Step 1 completed successfully before running this cell\n\nimport os\n\n# Verify JAVA_HOME is set\nif \"JAVA_HOME\" not in os.environ:\n    # Try to find and set it\n    for path in [\"/usr/lib/jvm/java-11-openjdk-amd64\", \"/usr/lib/jvm/java-8-openjdk-amd64\"]:\n        if os.path.exists(path):\n            os.environ[\"JAVA_HOME\"] = path\n            os.environ[\"PATH\"] = f\"{path}/bin:\" + os.environ.get(\"PATH\", \"\")\n            break\n\nprint(f\"JAVA_HOME: {os.environ.get('JAVA_HOME', 'NOT SET')}\")\n\nfrom pyspark.sql import SparkSession\nimport time\n\nspark = (\n    SparkSession.builder\n    .appName(\"MIS769_HW2\")\n    .master(\"local[2]\")  # Use 2 CPU cores\n    .config(\"spark.driver.memory\", \"2g\")  # Reduced for Colab compatibility\n    .config(\"spark.executor.memory\", \"2g\")\n    .getOrCreate()\n)\n\nprint(\"‚úÖ Spark Session Created!\")\nprint(f\"   App Name: {spark.sparkContext.appName}\")\nprint(f\"   Master: {spark.sparkContext.master}\")\nprint(f\"   Default Parallelism: {spark.sparkContext.defaultParallelism}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 1:** What does `local[2]` mean? What would `local[4]` do differently?\n",
    "\n",
    "*Your answer here:*\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Data Partitioning (5 points)\n",
    "\n",
    "### 2.1 Create Sample Data and Partition It"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# Create sample data: 100,000 random numbers\n",
    "data_list = [random.randint(1, 1000) for _ in range(100000)]\n",
    "print(f\"Created {len(data_list):,} data points\")\n",
    "\n",
    "# Create RDD with 4 partitions\n",
    "rdd = spark.sparkContext.parallelize(data_list, 4)\n",
    "print(f\"Number of partitions: {rdd.getNumPartitions()}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize how data is distributed across partitions\n",
    "def count_partition(index, iterator):\n",
    "    count = sum(1 for _ in iterator)\n",
    "    yield (index, count)\n",
    "\n",
    "partition_counts = rdd.mapPartitionsWithIndex(count_partition).collect()\n",
    "\n",
    "print(\"\\nüìä DATA DISTRIBUTION ACROSS PARTITIONS\")\n",
    "print(\"-\" * 40)\n",
    "for partition_id, count in partition_counts:\n",
    "    bar = \"‚ñà\" * (count // 1000)\n",
    "    print(f\"Partition {partition_id}: {count:,} items {bar}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 The MapReduce Pattern\n",
    "\n",
    "Let's see MapReduce in action: count how many times each number appears."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# MapReduce: Count frequency of each number\n",
    "# MAP: Transform each number to (number, 1)\n",
    "# REDUCE: Sum up all the 1s for each number\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# The MapReduce pattern\n",
    "frequency = (\n",
    "    rdd\n",
    "    .map(lambda x: (x, 1))           # MAP: (number, 1)\n",
    "    .reduceByKey(lambda a, b: a + b)  # REDUCE: sum counts\n",
    ")\n",
    "\n",
    "# Get top 10 most frequent numbers\n",
    "top_10 = frequency.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(\"üìä TOP 10 MOST FREQUENT NUMBERS\")\n",
    "print(\"-\" * 30)\n",
    "for num, count in top_10:\n",
    "    print(f\"   Number {num:4d}: {count:4d} times\")\n",
    "print(f\"\\n‚è±Ô∏è Time taken: {elapsed:.3f} seconds\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Part 3: Performance Experiment (5 points)\n",
    "\n",
    "Let's measure how the number of cores affects processing speed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Function to run our MapReduce with different core counts\n",
    "def run_experiment(num_cores, data_size=500000):\n",
    "    \"\"\"Run MapReduce with specified number of cores and return timing.\"\"\"\n",
    "\n",
    "    # Stop existing session\n",
    "    spark.stop()\n",
    "\n",
    "    # Create new session with specified cores\n",
    "    test_spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(f\"Test_{num_cores}_cores\")\n",
    "        .master(f\"local[{num_cores}]\")\n",
    "        .config(\"spark.driver.memory\", \"4g\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    # Create test data\n",
    "    test_data = [random.randint(1, 10000) for _ in range(data_size)]\n",
    "    test_rdd = test_spark.sparkContext.parallelize(test_data, num_cores * 2)\n",
    "\n",
    "    # Time the MapReduce operation\n",
    "    start = time.time()\n",
    "\n",
    "    result = (\n",
    "        test_rdd\n",
    "        .map(lambda x: (x, 1))\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "        .count()  # Force execution\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    test_spark.stop()\n",
    "    return elapsed\n",
    "\n",
    "print(\"Running performance experiment...\")\n",
    "print(\"(This may take a minute)\\n\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run experiments with 1, 2, and 4 cores\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = {}\n",
    "for cores in [1, 2, 4]:\n",
    "    print(f\"Testing with {cores} core(s)...\", end=\" \")\n",
    "    time_taken = run_experiment(cores)\n",
    "    results[cores] = time_taken\n",
    "    print(f\"{time_taken:.3f} seconds\")\n",
    "\n",
    "# Recreate spark session for later use\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MIS769_HW2\")\n",
    "    .master(\"local[2]\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize the results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "cores = list(results.keys())\n",
    "times = list(results.values())\n",
    "\n",
    "# Time comparison\n",
    "axes[0].bar(cores, times, color=['#e74c3c', '#f39c12', '#27ae60'])\n",
    "axes[0].set_xlabel('Number of Cores')\n",
    "axes[0].set_ylabel('Time (seconds)')\n",
    "axes[0].set_title('Processing Time by Core Count')\n",
    "axes[0].set_xticks(cores)\n",
    "\n",
    "# Speedup comparison\n",
    "baseline = results[1]\n",
    "speedups = [baseline / t for t in times]\n",
    "axes[1].bar(cores, speedups, color=['#e74c3c', '#f39c12', '#27ae60'])\n",
    "axes[1].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Number of Cores')\n",
    "axes[1].set_ylabel('Speedup (x times faster)')\n",
    "axes[1].set_title('Speedup Relative to 1 Core')\n",
    "axes[1].set_xticks(cores)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä PERFORMANCE SUMMARY\")\n",
    "print(\"-\" * 40)\n",
    "for c, t in results.items():\n",
    "    speedup = baseline / t\n",
    "    print(f\"{c} core(s): {t:.3f}s (speedup: {speedup:.2f}x)\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 2:** Why doesn't 4 cores give exactly 4x speedup? What factors limit the speedup?\n",
    "\n",
    "*Your answer here:*\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Part 4: Real Data Clustering with Spark ML (5 points)\n",
    "\n",
    "### 4.1 Load a Real Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Install datasets library\n",
    "!pip install datasets -q\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load Netflix dataset from HuggingFace\n",
    "print(\"Loading Netflix dataset...\")\n",
    "netflix_data = load_dataset(\"huggingfacejs/netflix-dataset\", split=\"train\")\n",
    "df_pandas = netflix_data.to_pandas()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df_pandas):,} Netflix titles\")\n",
    "df_pandas.head()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert to Spark DataFrame\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "print(f\"‚úÖ Converted to Spark DataFrame\")\n",
    "print(f\"   Partitions: {df_spark.rdd.getNumPartitions()}\")\n",
    "df_spark.printSchema()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 Prepare Features for Clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, when, year, length\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Create numeric features for clustering\n",
    "df_features = df_spark.select(\n",
    "    col(\"title\"),\n",
    "    col(\"type\"),\n",
    "    col(\"release_year\").cast(\"int\").alias(\"release_year\"),\n",
    "    length(col(\"description\")).alias(\"description_length\")\n",
    ").dropna()\n",
    "\n",
    "# Add binary feature for type\n",
    "df_features = df_features.withColumn(\n",
    "    \"is_movie\",\n",
    "    when(col(\"type\") == \"Movie\", 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Prepared {df_features.count():,} records for clustering\")\n",
    "df_features.show(5)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Assemble features into a vector\n",
    "feature_cols = [\"release_year\", \"description_length\", \"is_movie\"]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Apply transformations\n",
    "df_assembled = assembler.transform(df_features)\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "print(\"‚úÖ Features assembled and scaled\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3 Run K-Means Clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Train K-Means model with 3 clusters\n",
    "kmeans = KMeans(\n",
    "    k=3,\n",
    "    seed=42,\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"cluster\"\n",
    ")\n",
    "\n",
    "model = kmeans.fit(df_scaled)\n",
    "\n",
    "# Get predictions\n",
    "predictions = model.transform(df_scaled)\n",
    "\n",
    "print(\"‚úÖ K-Means clustering complete!\")\n",
    "print(f\"   Number of clusters: {model.getK()}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Analyze clusters\n",
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "cluster_stats = predictions.groupBy(\"cluster\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"release_year\").alias(\"avg_year\"),\n",
    "    avg(\"description_length\").alias(\"avg_desc_length\"),\n",
    "    avg(\"is_movie\").alias(\"pct_movies\")\n",
    ").orderBy(\"cluster\")\n",
    "\n",
    "print(\"\\nüìä CLUSTER ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "cluster_stats.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Sample titles from each cluster\n",
    "print(\"\\nüì∫ SAMPLE TITLES FROM EACH CLUSTER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for cluster_id in range(3):\n",
    "    print(f\"\\n--- Cluster {cluster_id} ---\")\n",
    "    samples = predictions.filter(col(\"cluster\") == cluster_id).select(\"title\", \"type\", \"release_year\").limit(5)\n",
    "    samples.show(truncate=False)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 3:** Based on your cluster analysis, what characterizes each cluster? Give each cluster a descriptive name (e.g., \"Recent TV Shows\", \"Classic Movies\", etc.)\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "- Cluster 0: \n",
    "- Cluster 1: \n",
    "- Cluster 2: \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Part 5: Draw Your Own Diagram (2 points)\n",
    "\n",
    "Create a simple diagram (can be ASCII art, drawing, or description) showing how Spark processes your Netflix clustering job.\n",
    "\n",
    "Include:\n",
    "- How data is split across partitions\n",
    "- What happens during the Map phase\n",
    "- What happens during the Reduce/Aggregate phase"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR DIAGRAM HERE (as code comments, text, or create an image)\n",
    "\n",
    "print(\"\"\"\n",
    "MY SPARK PROCESSING DIAGRAM:\n",
    "============================\n",
    "\n",
    "[Draw or describe your diagram here]\n",
    "\n",
    "Example structure:\n",
    "\n",
    "Netflix Data (8,800 titles)\n",
    "        |\n",
    "        v\n",
    "   [PARTITION]\n",
    "   /    |    \\\n",
    "  P1   P2    P3  (2,900 titles each)\n",
    "  |    |     |\n",
    "  v    v     v\n",
    " [MAP: Extract Features]\n",
    "  |    |     |\n",
    "  v    v     v\n",
    " [K-MEANS ITERATION]\n",
    "  \\    |    /\n",
    "   \\   |   /\n",
    "    v  v  v\n",
    "  [AGGREGATE: Update Centroids]\n",
    "        |\n",
    "        v\n",
    "  3 Final Clusters\n",
    "\n",
    "\"\"\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Clean Up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"‚úÖ Spark session stopped. Notebook complete!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "| Item | Points | Done? |\n",
    "|------|--------|-------|\n",
    "| Part 1: Spark setup complete | 3 | ‚òê |\n",
    "| Part 2: Data partitioning demonstrated | 5 | ‚òê |\n",
    "| Part 3: Performance experiment with analysis | 5 | ‚òê |\n",
    "| Part 4: K-Means clustering on Netflix data | 5 | ‚òê |\n",
    "| Part 5: Diagram explaining distributed processing | 2 | ‚òê |\n",
    "| **Total** | **20** | |\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [How Netflix Uses Spark](https://netflixtechblog.com/spark-and-spark-streaming-at-netflix-21e9e5e3cd44)\n",
    "- [K-Means Clustering Explained](https://scikit-learn.org/stable/modules/clustering.html#k-means)"
   ],
   "metadata": {}
  }
 ]
}