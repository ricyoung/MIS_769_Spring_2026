{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# ‚ö° Homework 2: MapReduce Concepts & Spark Fundamentals\n**MIS 769 - Big Data Analytics for Business | Spring 2026**\n\n**Points:** 20 | **Due:** Sunday, February 8, 2026 @ 11pm Pacific\n\n**Author:** Richard Young, Ph.D. | UNLV Lee Business School\n\n**Compute:** CPU (free tier)\n\n---\n\n## What You'll Learn\n\n1. Set up Apache Spark on Google Colab\n2. Understand how Spark partitions data for parallel processing\n3. **Measure and compare** processing performance with different configurations\n4. Apply K-Means clustering and interpret business results\n5. **Draw your own diagram** explaining distributed computing\n\n---\n\n## The Big Picture\n\nWhen data gets too big for one computer, we split the work across many computers. **Spark** is the industry-standard tool for this.\n\n```\nYOUR LAPTOP                           SPARK CLUSTER\n+----------------------+            +---------------------------+\n|                      |            |     Driver Program        |\n|   1 million rows     |            |         +----+            |\n|   Takes: 10 minutes  |   ------>  |         |    |            |\n|                      |            |    +----+----+----+       |\n+----------------------+            |    |    |    |    |       |\n                                    |   W1   W2   W3   W4       |\n                                    |  250K 250K 250K 250K      |\n                                    |  each worker in parallel  |\n                                    +---------------------------+\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Part 1: Spark Environment Setup (3 points)\n",
    "\n",
    "Let's install and configure Apache Spark on Google Colab."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Step 1: Install Java, PySpark, and findspark\n# Run this cell and WAIT for it to complete before proceeding\n\nimport os\nimport subprocess\n\n# Install Java 11\nprint(\"üì¶ Installing Java 11...\")\nos.system(\"apt-get update -qq && apt-get install -y -qq openjdk-11-jdk-headless > /dev/null 2>&1\")\n\n# Set JAVA_HOME\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\nos.environ[\"PATH\"] = f\"{os.environ['JAVA_HOME']}/bin:\" + os.environ[\"PATH\"]\n\n# Verify Java\njava_check = subprocess.run([\"java\", \"-version\"], capture_output=True, text=True)\nprint(f\"‚úÖ Java installed: {java_check.stderr.split(chr(10))[0]}\")\n\n# Install PySpark and findspark (specific versions for compatibility)\nprint(\"üì¶ Installing PySpark...\")\nos.system(\"pip install -q pyspark==3.5.0 findspark\")\n\n# Initialize findspark\nimport findspark\nfindspark.init()\n\nprint(\"‚úÖ Setup complete! You can now run the next cell.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 2: Create a Spark Session\n# Make sure Step 1 completed successfully first!\n\nfrom pyspark.sql import SparkSession\nimport time\n\nspark = (\n    SparkSession.builder\n    .appName(\"MIS769_HW2\")\n    .master(\"local[2]\")  # Use 2 CPU cores\n    .config(\"spark.driver.memory\", \"2g\")\n    .config(\"spark.ui.showConsoleProgress\", \"false\")  # Cleaner output\n    .getOrCreate()\n)\n\nprint(\"‚úÖ Spark Session Created!\")\nprint(f\"   App Name: {spark.sparkContext.appName}\")\nprint(f\"   Master: {spark.sparkContext.master}\")\nprint(f\"   Default Parallelism: {spark.sparkContext.defaultParallelism}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 1:** What does `local[2]` mean? What would `local[4]` do differently?\n",
    "\n",
    "*Your answer here:*\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Data Partitioning (5 points)\n",
    "\n",
    "### 2.1 Create Sample Data and Partition It"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# Create sample data: 100,000 random numbers\n",
    "data_list = [random.randint(1, 1000) for _ in range(100000)]\n",
    "print(f\"Created {len(data_list):,} data points\")\n",
    "\n",
    "# Create RDD with 4 partitions\n",
    "rdd = spark.sparkContext.parallelize(data_list, 4)\n",
    "print(f\"Number of partitions: {rdd.getNumPartitions()}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize how data is distributed across partitions\n",
    "def count_partition(index, iterator):\n",
    "    count = sum(1 for _ in iterator)\n",
    "    yield (index, count)\n",
    "\n",
    "partition_counts = rdd.mapPartitionsWithIndex(count_partition).collect()\n",
    "\n",
    "print(\"\\nüìä DATA DISTRIBUTION ACROSS PARTITIONS\")\n",
    "print(\"-\" * 40)\n",
    "for partition_id, count in partition_counts:\n",
    "    bar = \"‚ñà\" * (count // 1000)\n",
    "    print(f\"Partition {partition_id}: {count:,} items {bar}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 The MapReduce Pattern\n",
    "\n",
    "Let's see MapReduce in action: count how many times each number appears."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# MapReduce: Count frequency of each number\n",
    "# MAP: Transform each number to (number, 1)\n",
    "# REDUCE: Sum up all the 1s for each number\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# The MapReduce pattern\n",
    "frequency = (\n",
    "    rdd\n",
    "    .map(lambda x: (x, 1))           # MAP: (number, 1)\n",
    "    .reduceByKey(lambda a, b: a + b)  # REDUCE: sum counts\n",
    ")\n",
    "\n",
    "# Get top 10 most frequent numbers\n",
    "top_10 = frequency.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(\"üìä TOP 10 MOST FREQUENT NUMBERS\")\n",
    "print(\"-\" * 30)\n",
    "for num, count in top_10:\n",
    "    print(f\"   Number {num:4d}: {count:4d} times\")\n",
    "print(f\"\\n‚è±Ô∏è Time taken: {elapsed:.3f} seconds\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### üí° Try It Yourself: Search for Your Favorite Number\n\nHow would you search for a specific number? For example, if you want to find how many times the number **103** appears, you could filter the frequency RDD:\n\n```python\n# Find count for a specific number\nmy_number = 103\nresult = frequency.filter(lambda x: x[0] == my_number).collect()\nif result:\n    print(f\"Number {my_number} appears {result[0][1]} times\")\nelse:\n    print(f\"Number {my_number} not found\")\n```\n\n**Try it:** Change `my_number` to your favorite number and run the code above!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Try it! Search for your favorite number\nmy_number = 103  # <-- Change this to any number between 1-1000\n\nresult = frequency.filter(lambda x: x[0] == my_number).collect()\nif result:\n    print(f\"üîç Number {my_number} appears {result[0][1]} times\")\nelse:\n    print(f\"üîç Number {my_number} not found in the dataset\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Part 3: Performance Experiment (5 points)\n",
    "\n",
    "Let's measure how the number of cores affects processing speed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Function to run our MapReduce with different core counts\n",
    "def run_experiment(num_cores, data_size=500000):\n",
    "    \"\"\"Run MapReduce with specified number of cores and return timing.\"\"\"\n",
    "\n",
    "    # Stop existing session\n",
    "    spark.stop()\n",
    "\n",
    "    # Create new session with specified cores\n",
    "    test_spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(f\"Test_{num_cores}_cores\")\n",
    "        .master(f\"local[{num_cores}]\")\n",
    "        .config(\"spark.driver.memory\", \"4g\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    # Create test data\n",
    "    test_data = [random.randint(1, 10000) for _ in range(data_size)]\n",
    "    test_rdd = test_spark.sparkContext.parallelize(test_data, num_cores * 2)\n",
    "\n",
    "    # Time the MapReduce operation\n",
    "    start = time.time()\n",
    "\n",
    "    result = (\n",
    "        test_rdd\n",
    "        .map(lambda x: (x, 1))\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "        .count()  # Force execution\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    test_spark.stop()\n",
    "    return elapsed\n",
    "\n",
    "print(\"Running performance experiment...\")\n",
    "print(\"(This may take a minute)\\n\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run experiments with 1, 2, and 4 cores\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = {}\n",
    "for cores in [1, 2, 4]:\n",
    "    print(f\"Testing with {cores} core(s)...\", end=\" \")\n",
    "    time_taken = run_experiment(cores)\n",
    "    results[cores] = time_taken\n",
    "    print(f\"{time_taken:.3f} seconds\")\n",
    "\n",
    "# Recreate spark session for later use\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MIS769_HW2\")\n",
    "    .master(\"local[2]\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize the results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "cores = list(results.keys())\n",
    "times = list(results.values())\n",
    "\n",
    "# Time comparison\n",
    "axes[0].bar(cores, times, color=['#e74c3c', '#f39c12', '#27ae60'])\n",
    "axes[0].set_xlabel('Number of Cores')\n",
    "axes[0].set_ylabel('Time (seconds)')\n",
    "axes[0].set_title('Processing Time by Core Count')\n",
    "axes[0].set_xticks(cores)\n",
    "\n",
    "# Speedup comparison\n",
    "baseline = results[1]\n",
    "speedups = [baseline / t for t in times]\n",
    "axes[1].bar(cores, speedups, color=['#e74c3c', '#f39c12', '#27ae60'])\n",
    "axes[1].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Number of Cores')\n",
    "axes[1].set_ylabel('Speedup (x times faster)')\n",
    "axes[1].set_title('Speedup Relative to 1 Core')\n",
    "axes[1].set_xticks(cores)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä PERFORMANCE SUMMARY\")\n",
    "print(\"-\" * 40)\n",
    "for c, t in results.items():\n",
    "    speedup = baseline / t\n",
    "    print(f\"{c} core(s): {t:.3f}s (speedup: {speedup:.2f}x)\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 2:** Why doesn't 4 cores give exactly 4x speedup? What factors limit the speedup?\n",
    "\n",
    "*Your answer here:*\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### üåê From Laptop to Data Center: Spark's True Power\n\nIn this homework, we used `local[N]` which runs Spark on **multiple cores within your single computer**. This is great for learning, but here's the exciting part:\n\n**The same code scales to hundreds of computers without changes!**\n\n```\nLOCAL MODE (What we did)              CLUSTER MODE (Production)\n+------------------------+            +----------------------------------+\n|    YOUR LAPTOP         |            |         DATA CENTER              |\n|  +------------------+  |            |  +--------+  +--------+          |\n|  | Core 1 | Core 2  |  |            |  |Server 1|  |Server 2|          |\n|  |  P1    |   P2    |  |            |  | 64 GB  |  | 64 GB  |          |\n|  +------------------+  |            |  +--------+  +--------+          |\n|  | Core 3 | Core 4  |  |            |  +--------+  +--------+          |\n|  |  P3    |   P4    |  |            |  |Server 3|  |Server 4|          |\n|  +------------------+  |            |  | 64 GB  |  | 64 GB  |          |\n+------------------------+            |  +--------+  +--------+          |\n   .master(\"local[4]\")                |        ...100+ servers           |\n   Max: ~16GB RAM                     +----------------------------------+\n   Max: ~8 cores                         .master(\"spark://cluster:7077\")\n                                         Max: Petabytes, 1000s of cores\n```\n\n**Configuration Options:**\n| Mode | Setting | Use Case |\n|------|---------|----------|\n| Local (1 core) | `local` | Testing |\n| Local (N cores) | `local[4]` | Development (this homework) |\n| Local (all cores) | `local[*]` | Max local performance |\n| Cluster | `spark://master:7077` | Production clusters |\n| Cloud | `yarn` or `k8s` | AWS EMR, Databricks, GCP Dataproc |\n\n**The key insight:** Your MapReduce code from Part 2 would work identically on a 1000-node cluster processing terabytes of data. That's the power of Spark's abstraction!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 4: Real Data Clustering with Spark ML (5 points)\n\n### 4.1 Create a Streaming Content Dataset\n\nWe'll generate a realistic dataset of 5,000 streaming titles (similar to Netflix) to demonstrate K-Means clustering at scale.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a realistic streaming content dataset\n# (More reliable than external datasets that may be removed)\nimport pandas as pd\nimport random\n\n# Generate synthetic streaming data similar to Netflix\nrandom.seed(103)\n\ntitles_movies = [\n    \"The Last Voyage\", \"City of Dreams\", \"Dark Waters\", \"The Forgotten Path\",\n    \"Midnight Sun\", \"Silent Echo\", \"The Great Escape\", \"Beyond Tomorrow\",\n    \"Crimson Peak\", \"The Final Chapter\", \"Lost in Time\", \"Edge of Reality\",\n    \"The Hidden Truth\", \"Broken Promises\", \"Endless Night\", \"Golden Hour\",\n    \"The Perfect Storm\", \"Shadows Fall\", \"Rising Phoenix\", \"Distant Shores\"\n]\n\ntitles_tv = [\n    \"Crime Files\", \"The Office Life\", \"Mystery Manor\", \"Tech Giants\",\n    \"Family Ties\", \"Hospital Drama\", \"Legal Eagles\", \"Space Frontier\",\n    \"Comedy Hour\", \"Reality Check\", \"Cooking Masters\", \"Nature Wild\",\n    \"Documentary Now\", \"Teen Dreams\", \"Action Squad\", \"Thriller Zone\"\n]\n\ngenres = [\"Drama\", \"Comedy\", \"Action\", \"Documentary\", \"Thriller\", \"Romance\", \"Sci-Fi\", \"Horror\"]\n\n# Generate 5000 streaming titles\ndata = []\nfor i in range(5000):\n    if random.random() < 0.6:  # 60% movies\n        title_base = random.choice(titles_movies)\n        content_type = \"Movie\"\n        title = f\"{title_base} {random.randint(1, 99)}\" if random.random() < 0.3 else title_base\n    else:  # 40% TV shows\n        title_base = random.choice(titles_tv)\n        content_type = \"TV Show\"\n        title = f\"{title_base}: Season {random.randint(1, 8)}\"\n    \n    year = random.choices(\n        range(1990, 2026),\n        weights=[1]*10 + [2]*10 + [4]*10 + [8]*6,  # More recent = more likely\n        k=1\n    )[0]\n    \n    desc_length = random.randint(50, 500)\n    description = \"A \" + \" \".join([\"word\"] * (desc_length // 5))\n    \n    data.append({\n        \"title\": title,\n        \"type\": content_type,\n        \"release_year\": year,\n        \"genre\": random.choice(genres),\n        \"description\": description\n    })\n\ndf_pandas = pd.DataFrame(data)\n\nprint(f\"‚úÖ Created {len(df_pandas):,} streaming titles\")\nprint(f\"   Movies: {len(df_pandas[df_pandas['type']=='Movie']):,}\")\nprint(f\"   TV Shows: {len(df_pandas[df_pandas['type']=='TV Show']):,}\")\ndf_pandas.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert to Spark DataFrame\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "print(f\"‚úÖ Converted to Spark DataFrame\")\n",
    "print(f\"   Partitions: {df_spark.rdd.getNumPartitions()}\")\n",
    "df_spark.printSchema()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 Prepare Features for Clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, when, year, length\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Create numeric features for clustering\n",
    "df_features = df_spark.select(\n",
    "    col(\"title\"),\n",
    "    col(\"type\"),\n",
    "    col(\"release_year\").cast(\"int\").alias(\"release_year\"),\n",
    "    length(col(\"description\")).alias(\"description_length\")\n",
    ").dropna()\n",
    "\n",
    "# Add binary feature for type\n",
    "df_features = df_features.withColumn(\n",
    "    \"is_movie\",\n",
    "    when(col(\"type\") == \"Movie\", 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Prepared {df_features.count():,} records for clustering\")\n",
    "df_features.show(5)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Assemble features into a vector\n",
    "feature_cols = [\"release_year\", \"description_length\", \"is_movie\"]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Apply transformations\n",
    "df_assembled = assembler.transform(df_features)\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "print(\"‚úÖ Features assembled and scaled\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3 Run K-Means Clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Train K-Means model with 3 clusters\nkmeans = KMeans(\n    k=3,\n    seed=103,\n    featuresCol=\"features\",\n    predictionCol=\"cluster\"\n)\n\nmodel = kmeans.fit(df_scaled)\n\n# Get predictions\npredictions = model.transform(df_scaled)\n\nprint(\"‚úÖ K-Means clustering complete!\")\nprint(f\"   Number of clusters: {model.getK()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Analyze clusters\n",
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "cluster_stats = predictions.groupBy(\"cluster\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"release_year\").alias(\"avg_year\"),\n",
    "    avg(\"description_length\").alias(\"avg_desc_length\"),\n",
    "    avg(\"is_movie\").alias(\"pct_movies\")\n",
    ").orderBy(\"cluster\")\n",
    "\n",
    "print(\"\\nüìä CLUSTER ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "cluster_stats.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Sample titles from each cluster\n",
    "print(\"\\nüì∫ SAMPLE TITLES FROM EACH CLUSTER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for cluster_id in range(3):\n",
    "    print(f\"\\n--- Cluster {cluster_id} ---\")\n",
    "    samples = predictions.filter(col(\"cluster\") == cluster_id).select(\"title\", \"type\", \"release_year\").limit(5)\n",
    "    samples.show(truncate=False)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 3:** Based on your cluster analysis, what characterizes each cluster? Give each cluster a descriptive name (e.g., \"Recent TV Shows\", \"Classic Movies\", etc.)\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "- Cluster 0: \n",
    "- Cluster 1: \n",
    "- Cluster 2: \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 5: Draw Your Own Diagram (2 points)\n\n**The Big Takeaway:** Spark takes data and distributes it across cores, across computers, and across clusters. We use Spark when our data is too large to process on a single machine.\n\nCreate a simple diagram showing how Spark processes your streaming content clustering job.\n\n**Options for creating your diagram:**\n- ASCII art in the code cell below\n- Draw on paper and take a photo with your phone\n- Use any drawing tool (PowerPoint, Google Drawings, etc.)\n\n**Include in your diagram:**\n- How data is split across partitions (distributed to different workers)\n- What happens during the Map phase\n- What happens during the Reduce/Aggregate phase",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# YOUR DIAGRAM HERE (as code comments, text, or create an image)\n\nprint(\"\"\"\nMY SPARK PROCESSING DIAGRAM:\n============================\n\n[Draw or describe your diagram here]\n\nExample structure:\n\nStreaming Data (5,000 titles)\n        |\n        v\n   [PARTITION]\n   /    |    \\\\\n  P1   P2    P3  (~1,667 titles each)\n  |    |     |\n  v    v     v\n [MAP: Extract Features]\n  |    |     |\n  v    v     v\n [K-MEANS ITERATION]\n  \\\\    |    /\n   \\\\   |   /\n    v  v  v\n  [AGGREGATE: Update Centroids]\n        |\n        v\n  3 Final Clusters\n\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Clean Up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"‚úÖ Spark session stopped. Notebook complete!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "| Item | Points | Done? |\n",
    "|------|--------|-------|\n",
    "| Part 1: Spark setup complete | 3 | ‚òê |\n",
    "| Part 2: Data partitioning demonstrated | 5 | ‚òê |\n",
    "| Part 3: Performance experiment with analysis | 5 | ‚òê |\n",
    "| Part 4: K-Means clustering on Netflix data | 5 | ‚òê |\n",
    "| Part 5: Diagram explaining distributed processing | 2 | ‚òê |\n",
    "| **Total** | **20** | |\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [How Netflix Uses Spark](https://netflixtechblog.com/spark-and-spark-streaming-at-netflix-21e9e5e3cd44)\n",
    "- [K-Means Clustering Explained](https://scikit-learn.org/stable/modules/clustering.html#k-means)"
   ],
   "metadata": {}
  }
 ]
}