{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Homework 7: Build a RAG System\n",
    "**MIS 769 - Big Data Analytics for Business | Spring 2026**\n",
    "\n",
    "**Points:** 20 | **Due:** Sunday, March 29, 2026 @ 11pm Pacific\n",
    "\n",
    "**Author:** Richard Young, Ph.D. | UNLV Lee Business School\n",
    "\n",
    "**Compute:** CPU (free tier) ‚Äî GPU recommended\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. What RAG is and why it matters\n",
    "2. Build a document retrieval system using embeddings\n",
    "3. Integrate retrieval with a language model\n",
    "4. Evaluate RAG output quality\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Document Chunking (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers datasets faiss-cpu transformers -q\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\", split=\"train[:1000]\")\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df):,} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# Chunk all documents\n",
    "all_chunks = []\n",
    "chunk_to_doc = []  # Track which document each chunk came from\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    chunks = chunk_text(row['text'])\n",
    "    all_chunks.extend(chunks)\n",
    "    chunk_to_doc.extend([idx] * len(chunks))\n",
    "\n",
    "print(\"üìÑ DOCUMENT CHUNKING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original documents: {len(df):,}\")\n",
    "print(f\"Total chunks created: {len(all_chunks):,}\")\n",
    "print(f\"Average chunks per document: {len(all_chunks)/len(df):.1f}\")\n",
    "print(f\"Chunk size: 500 chars, Overlap: 50 chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Embedding Index (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Encoding chunks (this may take 1-2 minutes)...\")\n",
    "chunk_embeddings = model.encode(all_chunks, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Encoded {len(chunk_embeddings):,} chunks\")\n",
    "print(f\"Embedding dimension: {chunk_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS index\n",
    "dimension = chunk_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(chunk_embeddings).astype('float32'))\n",
    "\n",
    "print(\"üìä VECTOR INDEX BUILT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Embeddings shape: {chunk_embeddings.shape}\")\n",
    "print(f\"Index type: FAISS (Flat L2)\")\n",
    "print(f\"Index size: {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Retrieval (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=5):\n",
    "    \"\"\"Retrieve top-k relevant chunks for a query.\"\"\"\n",
    "    query_embedding = model.encode([query])\n",
    "    distances, indices = index.search(\n",
    "        np.array(query_embedding).astype('float32'), k\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        results.append({\n",
    "            'chunk': all_chunks[idx],\n",
    "            'distance': distances[0][i],\n",
    "            'doc_id': chunk_to_doc[idx]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Test retrieval\n",
    "query = \"What do people say about the acting?\"\n",
    "\n",
    "print(\"üîé RETRIEVAL TEST\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "\n",
    "results = retrieve(query)\n",
    "\n",
    "print(\"Retrieved Chunks:\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    score = 1 / (1 + r['distance'])  # Convert distance to similarity\n",
    "    print(f\"\\n{i}. [Score: {score:.2f}] (Doc #{r['doc_id']})\")\n",
    "    print(f\"   {r['chunk'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Generation with RAG (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a small language model (works on CPU)\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\", \n",
    "    model=\"google/flan-t5-small\",\n",
    "    max_new_tokens=150\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Language model loaded (flan-t5-small)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(query, k=3):\n",
    "    \"\"\"Answer a question using RAG.\"\"\"\n",
    "    # Retrieve relevant chunks\n",
    "    chunks = retrieve(query, k=k)\n",
    "    \n",
    "    # Build context\n",
    "    context = \"\\n\\n\".join([c['chunk'][:300] for c in chunks])\n",
    "    \n",
    "    # Build prompt\n",
    "    prompt = f\"\"\"Based on the following movie reviews, answer the question.\n",
    "\n",
    "Reviews:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Generate answer\n",
    "    response = generator(prompt)[0]['generated_text']\n",
    "    \n",
    "    return {\n",
    "        'answer': response,\n",
    "        'context': chunks\n",
    "    }\n",
    "\n",
    "# Test RAG\n",
    "query = \"What do reviewers think about the movie's plot?\"\n",
    "\n",
    "print(\"üí¨ RAG RESPONSE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "\n",
    "result = rag_answer(query)\n",
    "\n",
    "print(\"Retrieved Context (3 chunks):\")\n",
    "for i, c in enumerate(result['context'], 1):\n",
    "    print(f\"  {i}. {c['chunk'][:80]}...\")\n",
    "\n",
    "print(f\"\\nGenerated Answer:\")\n",
    "print(f\"  {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RAG vs No-RAG\n",
    "query = \"What are common complaints in movie reviews?\"\n",
    "\n",
    "print(\"üìä RAG vs NO-RAG COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "\n",
    "# Without RAG\n",
    "print(\"WITHOUT RAG (model's own knowledge):\")\n",
    "no_rag = generator(f\"What are common complaints in movie reviews? Answer:\")\n",
    "print(f\"  {no_rag[0]['generated_text']}\")\n",
    "\n",
    "# With RAG\n",
    "print(\"\\nWITH RAG (grounded in actual reviews):\")\n",
    "rag_result = rag_answer(query)\n",
    "print(f\"  {rag_result['answer']}\")\n",
    "\n",
    "print(\"\\nüí° RAG answers are grounded in your actual data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluation (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple queries\n",
    "test_queries = [\n",
    "    \"What makes a good movie according to reviewers?\",\n",
    "    \"What do people say about special effects?\",\n",
    "    \"Are there any mentions of famous actors?\"\n",
    "]\n",
    "\n",
    "print(\"üìä RAG EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQ: {query}\")\n",
    "    result = rag_answer(query)\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Questions to Answer\n",
    "\n",
    "**Q1:** How did you decide on chunk size and overlap?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "**Q2:** Show an example where RAG improved the LLM's answer.\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "**Q3:** Show an example where retrieval failed. Why?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "**Q4:** How would you deploy this for a real business application?\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "| Item | Points | Done? |\n",
    "|------|--------|-------|\n",
    "| Part 1: Document chunking | 3 | ‚òê |\n",
    "| Part 2: Embedding index | 4 | ‚òê |\n",
    "| Part 3: Retrieval | 5 | ‚òê |\n",
    "| Part 4: RAG generation | 5 | ‚òê |\n",
    "| Part 5: Evaluation | 3 | ‚òê |\n",
    "| **Total** | **20** | |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
