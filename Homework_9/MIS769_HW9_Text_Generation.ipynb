{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Homework 9: Text Generation & HuggingFace Spaces\n",
    "**MIS 769 - Big Data Analytics for Business | Spring 2026**\n",
    "\n",
    "**Points:** 20 | **Due:** Sunday, April 12, 2026 @ 11pm Pacific\n",
    "\n",
    "**Author:** Richard Young, Ph.D. | UNLV Lee Business School\n",
    "\n",
    "**Compute:** GPU recommended ‚Äî Spaces deployment is free\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Use HuggingFace Transformers for text generation\n",
    "2. Understand generation parameters (temperature, top-k, top-p)\n",
    "3. Build an interactive demo with Gradio\n",
    "4. Deploy your application to HuggingFace Spaces\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text Generation (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch gradio -q\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load GPT-2 (small, works on CPU)\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "print(\"‚úÖ GPT-2 model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text generation\n",
    "prompt = \"The future of artificial intelligence\"\n",
    "\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    pad_token_id=50256  # GPT-2 fix\n",
    ")\n",
    "\n",
    "print(\"üìù TEXT GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"\\nGenerated:\")\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parameter Exploration (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature comparison\n",
    "print(\"üìä TEMPERATURE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "temperatures = [0.3, 0.7, 1.2]\n",
    "\n",
    "for temp in temperatures:\n",
    "    output = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=30,\n",
    "        temperature=temp,\n",
    "        do_sample=True,\n",
    "        pad_token_id=50256\n",
    "    )\n",
    "    generated = output[0]['generated_text'][len(prompt):]\n",
    "    print(f\"\\nTemperature {temp}:\")\n",
    "    print(f\"  {generated[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-k and Top-p comparison\n",
    "print(\"üìä TOP-K AND TOP-P COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "configs = [\n",
    "    {\"top_k\": 10, \"top_p\": 1.0, \"name\": \"top_k=10\"},\n",
    "    {\"top_k\": 50, \"top_p\": 1.0, \"name\": \"top_k=50\"},\n",
    "    {\"top_k\": 0, \"top_p\": 0.9, \"name\": \"top_p=0.9\"},\n",
    "    {\"top_k\": 0, \"top_p\": 0.5, \"name\": \"top_p=0.5\"},\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "    output = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=30,\n",
    "        top_k=cfg[\"top_k\"] if cfg[\"top_k\"] > 0 else None,\n",
    "        top_p=cfg[\"top_p\"],\n",
    "        do_sample=True,\n",
    "        pad_token_id=50256\n",
    "    )\n",
    "    generated = output[0]['generated_text'][len(prompt):]\n",
    "    print(f\"\\n{cfg['name']}:\")\n",
    "    print(f\"  {generated[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table\n",
    "print(\"\\nüìä PARAMETER SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "| Parameter   | Effect                          | Typical Values |\n",
    "|-------------|--------------------------------|----------------|\n",
    "| temperature | Higher = more random           | 0.3-1.0        |\n",
    "| top_k       | Limits token choices           | 20-100         |\n",
    "| top_p       | Nucleus sampling threshold     | 0.8-0.95       |\n",
    "| max_tokens  | Maximum output length          | 50-500         |\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Gradio Interface (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def generate_text(prompt, temperature, max_length):\n",
    "    \"\"\"Generate text with specified parameters.\"\"\"\n",
    "    if not prompt:\n",
    "        return \"Please enter a prompt.\"\n",
    "    \n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=int(max_length),\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        pad_token_id=50256\n",
    "    )\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=generate_text,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Prompt\", placeholder=\"Enter your prompt here...\", lines=2),\n",
    "        gr.Slider(0.1, 1.5, value=0.7, step=0.1, label=\"Temperature\"),\n",
    "        gr.Slider(20, 200, value=50, step=10, label=\"Max New Tokens\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Generated Text\", lines=5),\n",
    "    title=\"üöÄ Text Generator\",\n",
    "    description=\"Generate creative text using GPT-2. Adjust temperature for creativity.\",\n",
    "    examples=[\n",
    "        [\"The future of technology is\", 0.7, 50],\n",
    "        [\"Once upon a time in a magical forest\", 0.9, 100],\n",
    "        [\"Dear customer, thank you for\", 0.5, 50]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"üé® GRADIO INTERFACE CREATED\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Run demo.launch() to start the interface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the demo (in Colab, this creates a public URL)\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: HuggingFace Spaces Deployment (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the app.py file for Spaces\n",
    "app_code = '''\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load model\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "def generate_text(prompt, temperature, max_length):\n",
    "    if not prompt:\n",
    "        return \"Please enter a prompt.\"\n",
    "    \n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=int(max_length),\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        pad_token_id=50256\n",
    "    )\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate_text,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Prompt\", placeholder=\"Enter your prompt...\"),\n",
    "        gr.Slider(0.1, 1.5, value=0.7, label=\"Temperature\"),\n",
    "        gr.Slider(20, 200, value=50, label=\"Max Tokens\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Generated Text\"),\n",
    "    title=\"Text Generator\",\n",
    "    description=\"Generate text with GPT-2\"\n",
    ")\n",
    "\n",
    "demo.launch()\n",
    "'''\n",
    "\n",
    "print(\"üìÑ APP.PY FOR HUGGINGFACE SPACES\")\n",
    "print(\"=\" * 60)\n",
    "print(app_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate requirements.txt\n",
    "requirements = \"\"\"transformers\n",
    "gradio\n",
    "torch\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìÑ REQUIREMENTS.TXT\")\n",
    "print(\"=\" * 60)\n",
    "print(requirements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Steps\n",
    "\n",
    "1. Go to [huggingface.co/spaces](https://huggingface.co/spaces)\n",
    "2. Click \"Create new Space\"\n",
    "3. Choose a name (e.g., `text-generator`)\n",
    "4. Select **Gradio** as the SDK\n",
    "5. Upload `app.py` and `requirements.txt`\n",
    "6. Wait for the build to complete\n",
    "7. **Copy your Space URL and paste it below!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record your Space URL\n",
    "space_url = \"https://huggingface.co/spaces/YOUR_USERNAME/YOUR_SPACE_NAME\"\n",
    "\n",
    "print(\"üöÄ YOUR HUGGINGFACE SPACE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"URL: {space_url}\")\n",
    "print(\"\\n‚ö†Ô∏è Replace with your actual Space URL before submitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Documentation (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various generation scenarios\n",
    "test_prompts = [\n",
    "    \"Write a professional email starting with: Dear hiring manager,\",\n",
    "    \"Create a product description for: A smart water bottle that\",\n",
    "    \"Generate a tweet about: The benefits of remote work\"\n",
    "]\n",
    "\n",
    "print(\"üìä GENERATION SHOWCASE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    output = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=50256\n",
    "    )\n",
    "    print(f\"\\nPrompt: {prompt[:50]}...\")\n",
    "    print(f\"Output: {output[0]['generated_text'][len(prompt):][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Questions to Answer\n",
    "\n",
    "**Q1:** How do temperature and top-p affect output creativity?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "**Q2:** What trade-offs did you make for Spaces deployment?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "**Q3:** What would you add to make this production-ready?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "**Q4:** Your HuggingFace Spaces URL:\n",
    "\n",
    "*Your URL:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "| Item | Points | Done? |\n",
    "|------|--------|-------|\n",
    "| Part 1: Text generation | 4 | ‚òê |\n",
    "| Part 2: Parameter exploration | 4 | ‚òê |\n",
    "| Part 3: Gradio interface | 5 | ‚òê |\n",
    "| Part 4: Spaces deployment | 5 | ‚òê |\n",
    "| Part 5: Documentation | 2 | ‚òê |\n",
    "| **Total** | **20** | |\n",
    "\n",
    "**Don't forget to include your HuggingFace Spaces URL!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
